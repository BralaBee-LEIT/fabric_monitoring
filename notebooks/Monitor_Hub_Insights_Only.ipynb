{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b7fa076",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Setup & Configuration\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from IPython.display import display\n",
    "from pyspark.sql.functions import col, to_timestamp, unix_timestamp, coalesce, initcap, regexp_replace, element_at, split, when, lit, count, avg, max as max_, date_trunc\n",
    "\n",
    "# Configure Plotting Style\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_context(\"notebook\", font_scale=1.2)\n",
    "\n",
    "# PATH CONFIGURATION\n",
    "# We use the relative path 'Files/...' which maps correctly in Fabric Lakehouses\n",
    "# This avoids the 'Spark_Ambiguous_MsSparkUtils_UseMountedPathFailure' error\n",
    "CSV_PATH = \"Files/monitor_hub_analysis\"\n",
    "print(f\"üìÇ Data Source Path: {CSV_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbcd5977",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Load Data\n",
    "try:\n",
    "    # Match the master activities report\n",
    "    path_pattern = f\"{CSV_PATH}/activities_master_*.csv\"\n",
    "    print(f\"‚è≥ Loading data from {path_pattern}...\")\n",
    "    \n",
    "    # Read CSV with header & inferSchema\n",
    "    # inferSchema is important to correctly detect Integers and Timestamps\n",
    "    raw_df = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(path_pattern)\n",
    "    \n",
    "    print(f\"‚úÖ Loaded {raw_df.count()} rows.\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error loading data: {e}\")\n",
    "    print(\"   Ensure that 'Monitor_Hub_Analysis.ipynb' has been run at least once to generate the CSVs.\")\n",
    "    raw_df = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc2a018c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Data Standardization\n",
    "# Map raw CSV columns to a clean schema for analysis\n",
    "# We use coalesce to handle potential column name variations between versions\n",
    "\n",
    "if raw_df:\n",
    "    def safe_col(c):\n",
    "        return col(c) if c in raw_df.columns else lit(None)\n",
    "\n",
    "    df = raw_df.select(\n",
    "        coalesce(safe_col(\"workspace_name\"), safe_col(\"WorkSpaceName\"), safe_col(\"workspace_id\")).alias(\"Workspace\"),\n",
    "        coalesce(safe_col(\"item_name\"), safe_col(\"ItemName\")).alias(\"Item_Name\"),\n",
    "        coalesce(safe_col(\"item_type\"), safe_col(\"ItemType\")).alias(\"Item_Type\"),\n",
    "        coalesce(safe_col(\"activity_type\"), safe_col(\"Operation\")).alias(\"Operation\"),\n",
    "        coalesce(safe_col(\"status\"), safe_col(\"Status\")).alias(\"Status\"),\n",
    "        coalesce(safe_col(\"start_time\"), safe_col(\"CreationTime\")).alias(\"Start_Time\"),\n",
    "        coalesce(safe_col(\"end_time\"), safe_col(\"EndTime\")).alias(\"End_Time\"),\n",
    "        coalesce(safe_col(\"duration_seconds\"), safe_col(\"Duration\")).cast(\"double\").alias(\"Duration_Sec\"),\n",
    "        coalesce(safe_col(\"submitted_by\"), safe_col(\"UserId\")).alias(\"User_ID\")\n",
    "    )\n",
    "    \n",
    "    # Cache for performance since we'll query this multiple times\n",
    "    df.cache()\n",
    "    df.createOrReplaceTempView(\"fabric_activities\")\n",
    "    \n",
    "    df.show(5)\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No data to process.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d95a23ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. High-Level KPIs\n",
    "\n",
    "if raw_df:\n",
    "    total_activities = df.count()\n",
    "    failed_activities = df.filter(col(\"Status\") == \"Failed\").count()\n",
    "    failure_rate = (failed_activities / total_activities) * 100 if total_activities > 0 else 0\n",
    "    \n",
    "    unique_users = df.select(\"User_ID\").distinct().count()\n",
    "    unique_workspaces = df.select(\"Workspace\").distinct().count()\n",
    "    \n",
    "    print(\"=\"*40)\n",
    "    print(\"üìä EXECUTIVE SUMMARY\")\n",
    "    print(\"=\"*40)\n",
    "    print(f\"Total Activities:    {total_activities:,}\")\n",
    "    print(f\"Total Failures:      {failed_activities:,}\")\n",
    "    print(f\"Failure Rate:        {failure_rate:.2f}%\")\n",
    "    print(f\"Active Users:        {unique_users:,}\")\n",
    "    print(f\"Active Workspaces:   {unique_workspaces:,}\")\n",
    "    print(\"=\"*40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75d43f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Trend Analysis (Daily Volume)\n",
    "\n",
    "if raw_df:\n",
    "    # Aggregate by Day\n",
    "    daily_trend = df.withColumn(\"Date\", date_trunc(\"day\", col(\"Start_Time\"))) \\\n",
    "        .groupBy(\"Date\", \"Status\") \\\n",
    "        .count() \\\n",
    "        .orderBy(\"Date\") \\\n",
    "        .toPandas()\n",
    "    \n",
    "    # Pivot for plotting\n",
    "    pivot_trend = daily_trend.pivot(index='Date', columns='Status', values='count').fillna(0)\n",
    "    \n",
    "    # Plot\n",
    "    plt.figure(figsize=(14, 6))\n",
    "    pivot_trend.plot(kind='bar', stacked=True, ax=plt.gca(), colormap='viridis')\n",
    "    plt.title('Daily Activity Volume by Status')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Count')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c3c7c7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Top Failing Workspaces\n",
    "\n",
    "if raw_df:\n",
    "    top_fail_workspaces = df.filter(col(\"Status\") == \"Failed\") \\\n",
    "        .groupBy(\"Workspace\") \\\n",
    "        .count() \\\n",
    "        .orderBy(col(\"count\").desc()) \\\n",
    "        .limit(10) \\\n",
    "        .toPandas()\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.barplot(data=top_fail_workspaces, x='count', y='Workspace', palette='Reds_r')\n",
    "    plt.title('Top 10 Workspaces by Failure Count')\n",
    "    plt.xlabel('Failures')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79be8cc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Longest Running Items (Performance Bottlenecks)\n",
    "\n",
    "if raw_df:\n",
    "    slowest_items = df.filter(col(\"Status\") == \"Succeeded\") \\\n",
    "        .select(\"Workspace\", \"Item_Name\", \"Operation\", \"Duration_Sec\") \\\n",
    "        .orderBy(col(\"Duration_Sec\").desc()) \\\n",
    "        .limit(10) \\\n",
    "        .toPandas()\n",
    "    \n",
    "    print(\"üê¢ Top 10 Slowest Successful Operations:\")\n",
    "    display(slowest_items)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
