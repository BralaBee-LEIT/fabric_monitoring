{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a7a4fad5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚öôÔ∏è Initializing Spark Session...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "25/12/03 01:51:33 WARN Utils: Your hostname, sanmi-System-Product-Name, resolves to a loopback address: 127.0.1.1; using 192.168.0.14 instead (on interface eno1)\n",
      "25/12/03 01:51:33 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "25/12/03 01:51:33 WARN Utils: Your hostname, sanmi-System-Product-Name, resolves to a loopback address: 127.0.1.1; using 192.168.0.14 instead (on interface eno1)\n",
      "25/12/03 01:51:33 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/12/03 01:51:34 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "25/12/03 01:51:34 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "25/12/03 01:51:34 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "25/12/03 01:51:34 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n",
      "25/12/03 01:51:34 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "25/12/03 01:51:34 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Spark Session Created: 4.0.1\n",
      "üîß Running in LOCAL mode.\n",
      "  - Item Details: /home/sanmi/Documents/J'TOYE_DIGITAL/LEIT_TEKSYSTEMS/1_Project_Rhico/usf_fabric_monitoring/exports/fabric_item_details\n",
      "  - Audit Logs:   /home/sanmi/Documents/J'TOYE_DIGITAL/LEIT_TEKSYSTEMS/1_Project_Rhico/usf_fabric_monitoring/exports/monitor_hub_analysis/raw_data/daily\n"
     ]
    }
   ],
   "source": [
    "# 1. Setup & Configuration\n",
    "import os\n",
    "import sys\n",
    "import glob\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, to_timestamp, when, count, desc, lit, unix_timestamp, coalesce, abs as abs_val\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType, TimestampType\n",
    "\n",
    "# Initialize Spark Session\n",
    "if 'spark' not in locals() or spark is None:\n",
    "    print(\"‚öôÔ∏è Initializing Spark Session...\")\n",
    "    spark = SparkSession.builder \\\n",
    "        .appName(\"FabricFailureAnalysis\") \\\n",
    "        .master(\"local[*]\") \\\n",
    "        .getOrCreate()\n",
    "    print(f\"‚úÖ Spark Session Created: {spark.version}\")\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "IS_LOCAL_TESTING = True\n",
    "\n",
    "if IS_LOCAL_TESTING:\n",
    "    # Point to the new detailed exports directory\n",
    "    BASE_PATH = os.path.abspath(os.path.join(os.getcwd(), \"../exports/fabric_item_details/\"))\n",
    "    # Point to the Audit Logs directory\n",
    "    AUDIT_LOG_PATH = os.path.abspath(os.path.join(os.getcwd(), \"../exports/monitor_hub_analysis/raw_data/daily/\"))\n",
    "    print(f\"üîß Running in LOCAL mode.\")\n",
    "    print(f\"  - Item Details: {BASE_PATH}\")\n",
    "    print(f\"  - Audit Logs:   {AUDIT_LOG_PATH}\")\n",
    "else:\n",
    "    BASE_PATH = \"Files/exports/fabric_item_details/\"\n",
    "    AUDIT_LOG_PATH = \"Files/exports/monitor_hub_analysis/raw_data/daily/\"\n",
    "    print(f\"‚òÅÔ∏è Running in FABRIC mode.\")\n",
    "    print(f\"  - Item Details: {BASE_PATH}\")\n",
    "    print(f\"  - Audit Logs:   {AUDIT_LOG_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e118063a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Define Schemas\n",
    "\n",
    "# JSON Schema for the detailed item exports\n",
    "# Note: failureReason is a nested object in the JSON source\n",
    "json_schema = StructType([\n",
    "    StructField(\"id\", StringType(), True),\n",
    "    StructField(\"itemId\", StringType(), True),\n",
    "    StructField(\"jobType\", StringType(), True),\n",
    "    StructField(\"invokeType\", StringType(), True),\n",
    "    StructField(\"status\", StringType(), True),\n",
    "    StructField(\"failureReason\", StructType([\n",
    "        StructField(\"errorCode\", StringType(), True),\n",
    "        StructField(\"message\", StringType(), True),\n",
    "        StructField(\"requestId\", StringType(), True)\n",
    "    ]), True),\n",
    "    StructField(\"rootActivityId\", StringType(), True),\n",
    "    StructField(\"startTimeUtc\", StringType(), True), # Read as String first to handle ISO format safely\n",
    "    StructField(\"endTimeUtc\", StringType(), True),\n",
    "    StructField(\"_workspace_name\", StringType(), True),\n",
    "    StructField(\"_item_name\", StringType(), True),\n",
    "    StructField(\"_item_type\", StringType(), True),\n",
    "    StructField(\"duration\", DoubleType(), True)\n",
    "])\n",
    "\n",
    "# Note on Principal/User Information:\n",
    "# The underlying source API (jobs/instances) does not currently return the 'principal' or 'userId' \n",
    "# responsible for the execution. This information is not available in the current dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f8726a40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading all failure data...\n",
      "üìÇ Loading 2 files from /home/sanmi/Documents/J'TOYE_DIGITAL/LEIT_TEKSYSTEMS/1_Project_Rhico/usf_fabric_monitoring/exports/fabric_item_details...\n",
      "Loading Audit Logs...\n",
      "Loading Audit Logs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/12/03 01:51:37 WARN FileStreamSink: Assume no metadata directory. Error while looking for metadata directory in the path: /home/sanmi/Documents/J'TOYE_DIGITAL/LEIT_TEKSYSTEMS/1_Project_Rhico/usf_fabric_monitoring/exports/monitor_hub_analysis/raw_data/daily/fabric_activities_*.csv.\n",
      "java.io.FileNotFoundException: File /home/sanmi/Documents/J'TOYE_DIGITAL/LEIT_TEKSYSTEMS/1_Project_Rhico/usf_fabric_monitoring/exports/monitor_hub_analysis/raw_data/daily/fabric_activities_*.csv does not exist\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:917)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1238)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:907)\n",
      "\tat org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:462)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileStreamSink$.hasMetadata(FileStreamSink.scala:56)\n",
      "\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:381)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource.org$apache$spark$sql$catalyst$analysis$ResolveDataSource$$loadV1BatchSource(ResolveDataSource.scala:143)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.$anonfun$applyOrElse$2(ResolveDataSource.scala:61)\n",
      "\tat scala.Option.getOrElse(Option.scala:201)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:61)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:45)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:139)\n",
      "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:86)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:139)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:416)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:135)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:131)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:37)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:112)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:111)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:37)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:45)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:43)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:242)\n",
      "\tat scala.collection.LinearSeqOps.foldLeft(LinearSeq.scala:183)\n",
      "\tat scala.collection.LinearSeqOps.foldLeft$(LinearSeq.scala:179)\n",
      "\tat scala.collection.immutable.List.foldLeft(List.scala:79)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:239)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:231)\n",
      "\tat scala.collection.immutable.List.foreach(List.scala:334)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:231)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:340)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:336)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:234)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:336)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:299)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:201)\n",
      "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:201)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:190)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:76)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:111)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:71)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:330)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:423)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:330)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:110)\n",
      "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:148)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:278)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:654)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:278)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:277)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:110)\n",
      "\tat scala.util.Try$.apply(Try.scala:217)\n",
      "\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1378)\n",
      "\tat org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:46)\n",
      "\tat org.apache.spark.util.LazyTry.tryT(LazyTry.scala:46)\n",
      "\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:58)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:121)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:80)\n",
      "\tat org.apache.spark.sql.classic.Dataset$.$anonfun$ofRows$1(Dataset.scala:115)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\n",
      "\tat org.apache.spark.sql.classic.Dataset$.ofRows(Dataset.scala:113)\n",
      "\tat org.apache.spark.sql.classic.DataFrameReader.load(DataFrameReader.scala:109)\n",
      "\tat org.apache.spark.sql.classic.DataFrameReader.load(DataFrameReader.scala:58)\n",
      "\tat org.apache.spark.sql.DataFrameReader.csv(DataFrameReader.scala:392)\n",
      "\tat org.apache.spark.sql.classic.DataFrameReader.csv(DataFrameReader.scala:259)\n",
      "\tat org.apache.spark.sql.classic.DataFrameReader.csv(DataFrameReader.scala:58)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:75)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:580)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîó Joining with Audit Logs to enrich User info...\n",
      "‚úÖ Successfully loaded 1203 failure records.\n",
      "‚úÖ Successfully loaded 1203 failure records.\n"
     ]
    }
   ],
   "source": [
    "# 3. Load and Process Data\n",
    "from pyspark.sql.functions import split, initcap, regexp_replace, element_at, substring\n",
    "\n",
    "def load_audit_logs():\n",
    "    \"\"\"\n",
    "    Loads the Fabric Audit Logs (Activity Events) to get User information.\n",
    "    \"\"\"\n",
    "    print(\"Loading Audit Logs...\")\n",
    "    try:\n",
    "        # Load all CSVs with header, letting Spark infer columns by name (all strings by default)\n",
    "        # Use PERMISSIVE mode to handle bad rows without crashing\n",
    "        audit_df = spark.read.option(\"header\", \"true\") \\\n",
    "            .option(\"mode\", \"PERMISSIVE\") \\\n",
    "            .csv(os.path.join(AUDIT_LOG_PATH, \"fabric_activities_*.csv\"))\n",
    "            \n",
    "        # Filter for relevant activities\n",
    "        # Also filter out rows where CreationTime is not a valid timestamp string (e.g. \"Succeeded\")\n",
    "        # This handles cases where CSV parsing might have shifted columns\n",
    "        audit_df = audit_df.filter(col(\"Activity\").isin(\"RunArtifact\", \"ExecuteNotebook\", \"ExecutePipeline\")) \\\n",
    "            .filter(col(\"CreationTime\").rlike(r\"^\\d{4}-\\d{2}-\\d{2}\"))\n",
    "            \n",
    "        # Clean CreationTime (remove \" UTC\" suffix if present) and cast to timestamp\n",
    "        # We use substring to take the first 19 characters (yyyy-MM-dd HH:mm:ss) which is safe if format is consistent\n",
    "        audit_df = audit_df.withColumn(\"AuditTime\", to_timestamp(substring(col(\"CreationTime\"), 1, 19)))\n",
    "            \n",
    "        # Select available columns\n",
    "        # We know UserId exists based on file inspection\n",
    "        select_cols = [\"ItemId\", \"SubmittedBy\", \"AuditTime\", \"UserId\"]\n",
    "            \n",
    "        audit_df = audit_df.select(*[col(c) for c in select_cols])\n",
    "            \n",
    "        return audit_df\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Could not load Audit Logs: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def load_and_process_failures(file_pattern=\"jobs_*.json\"):\n",
    "    \"\"\"\n",
    "    Loads JSONs matching the pattern, filters for failures.\n",
    "    \"\"\"\n",
    "    full_path_pattern = os.path.join(BASE_PATH, file_pattern)\n",
    "    matched_files = glob.glob(full_path_pattern)\n",
    "    \n",
    "    if not matched_files:\n",
    "        # Fallback to old pattern if new generic files aren't found yet\n",
    "        print(f\"‚ö†Ô∏è No files found at {full_path_pattern}. Trying legacy patterns...\")\n",
    "        matched_files = glob.glob(os.path.join(BASE_PATH, \"*.json\"))\n",
    "        if not matched_files:\n",
    "            print(f\"‚ö†Ô∏è No JSON files found at {BASE_PATH}\")\n",
    "            return None\n",
    "        \n",
    "    try:\n",
    "        print(f\"üìÇ Loading {len(matched_files)} files from {BASE_PATH}...\")\n",
    "        df = spark.read.option(\"multiLine\", \"true\").schema(json_schema).json(matched_files)\n",
    "        failed_df = df.filter(col(\"status\") == \"Failed\")\n",
    "        \n",
    "        parsed_df = failed_df \\\n",
    "            .withColumn(\"error_code\", col(\"failureReason.errorCode\")) \\\n",
    "            .withColumn(\"error_message\", col(\"failureReason.message\")) \\\n",
    "            .withColumn(\"source_type\", col(\"_item_type\")) \\\n",
    "            .withColumn(\"start_time\", to_timestamp(col(\"startTimeUtc\"))) \\\n",
    "            .withColumn(\"end_time\", to_timestamp(col(\"endTimeUtc\"))) \\\n",
    "            .withColumn(\"itemId\", col(\"itemId\")) \\\n",
    "            .withColumn(\"invokeType\", col(\"invokeType\")) # Added Invoke Type\n",
    "        \n",
    "        parsed_df = parsed_df.withColumn(\"calculated_duration\", \n",
    "            (unix_timestamp(col(\"end_time\")) - unix_timestamp(col(\"start_time\"))).cast(\"double\")\n",
    "        )\n",
    "            \n",
    "        return parsed_df\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Could not load data: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# Load all failure data from JSON exports\n",
    "print(\"Loading all failure data...\")\n",
    "# Updated default pattern to match the new generic output\n",
    "failures_df = load_and_process_failures(\"jobs_*.json\")\n",
    "\n",
    "# Join with Audit Logs\n",
    "final_df = None\n",
    "if failures_df:\n",
    "    audit_df = load_audit_logs()\n",
    "    if audit_df:\n",
    "        print(\"üîó Joining with Audit Logs to enrich User info...\")\n",
    "        \n",
    "        audit_renamed = audit_df.withColumnRenamed(\"ItemId\", \"AuditItemId\") \\\n",
    "                                .withColumnRenamed(\"AuditTime\", \"AuditTime\")\n",
    "        \n",
    "        joined_df = failures_df.join(audit_renamed, \n",
    "            (failures_df.itemId == audit_renamed.AuditItemId) & \n",
    "            (abs_val(unix_timestamp(failures_df.start_time) - unix_timestamp(audit_renamed.AuditTime)) < 120),\n",
    "            \"left\"\n",
    "        )\n",
    "        final_df = joined_df.dropDuplicates([\"id\"])\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è Audit logs not available. User info will be missing.\")\n",
    "        final_df = failures_df.withColumn(\"UserId\", lit(None)).withColumn(\"SubmittedBy\", lit(None))\n",
    "\n",
    "    # Final Selection with Enhanced Columns\n",
    "    final_df = final_df.select(\n",
    "        col(\"_workspace_name\").alias(\"Workspace\"),\n",
    "        col(\"_item_name\").alias(\"Item Name\"),\n",
    "        col(\"_item_type\").alias(\"Item Type\"),\n",
    "        col(\"invokeType\").alias(\"Invoke Type\"), # Added\n",
    "        col(\"start_time\").alias(\"Start Time\"),\n",
    "        col(\"end_time\").alias(\"End Time\"),\n",
    "        coalesce(col(\"duration\")/1000, col(\"calculated_duration\")).alias(\"Duration (s)\"),\n",
    "        col(\"UserId\").alias(\"User ID\"), # Original User ID\n",
    "        \n",
    "        # Enhanced User Name Extraction\n",
    "        # Tries to extract \"Firstname Lastname\" from \"firstname.lastname@domain.com\"\n",
    "        coalesce(\n",
    "            initcap(regexp_replace(element_at(split(col(\"UserId\"), \"@\"), 1), \"\\\\.\", \" \")),\n",
    "            col(\"SubmittedBy\"), \n",
    "            col(\"UserId\")\n",
    "        ).alias(\"User Name\"),\n",
    "        \n",
    "        col(\"error_code\").alias(\"Error Code\"),\n",
    "        col(\"error_message\").alias(\"Error Message\")\n",
    "    )\n",
    "\n",
    "    print(f\"‚úÖ Successfully loaded {final_df.count()} failure records.\")\n",
    "else:\n",
    "    print(\"‚ùå No failure data found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "953e901b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Data Coverage Diagnostics ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Range: 2025-11-02 01:00:02.893333 to 2025-12-02 23:00:03.223119\n",
      "Records with User Info: 932 / 1203\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Latest Record with User Info: 2025-11-30 23:50:00.643333\n",
      "‚ö†Ô∏è WARNING: No User Info available for failures after 2025-11-30 23:50:00.643333. Audit Logs may be missing for recent dates.\n",
      "\n",
      "=== Recent Failures (Detailed) ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------+-------------------------+------------+-----------+--------------------------+--------------------------+------------+-------+---------+-------------------+--------------------------------------------------+\n",
      "|Workspace                   |Item Name                |Item Type   |Invoke Type|Start Time                |End Time                  |Duration (s)|User ID|User Name|Error Code         |Error Msg (Trunc)                                 |\n",
      "+----------------------------+-------------------------+------------+-----------+--------------------------+--------------------------+------------+-------+---------+-------------------+--------------------------------------------------+\n",
      "|RE Service - Data Operations|MIf_Analytics_Contracts  |DataPipeline|Scheduled  |2025-12-02 23:00:03.223119|2025-12-02 23:16:11.739386|968.0       |NULL   |NULL     |Failed             |Operation on target install_base failed: Failed to|\n",
      "|rescm_dev_test              |JDE_BI_OUT               |Dataflow    |Manual     |2025-12-02 22:58:07.647734|2025-12-02 22:58:09.796666|2.0         |NULL   |NULL     |RefreshDedupedError|NULL                                              |\n",
      "|rescm_dev_test              |JDE_BIMIN05              |DataPipeline|Manual     |2025-12-02 22:57:50.604523|2025-12-02 22:58:26.365714|36.0        |NULL   |NULL     |Failed             |Operation on target JDE_BI_OUT failed: Dataflow re|\n",
      "|rescm_dev_test              |JDE_BI_OUT               |Dataflow    |Manual     |2025-12-02 22:52:23.122001|2025-12-02 22:52:25.653333|2.0         |NULL   |NULL     |RefreshDedupedError|NULL                                              |\n",
      "|rescm_dev_test              |JDE_BIMIN05              |DataPipeline|Manual     |2025-12-02 22:52:06.71024 |2025-12-02 22:52:41.776669|35.0        |NULL   |NULL     |Failed             |Operation on target JDE_BI_OUT failed: Dataflow re|\n",
      "|rescm_dev_test              |JDE_BI_OUT               |Dataflow    |Manual     |2025-12-02 22:47:40.588642|2025-12-02 22:47:42.95    |2.0         |NULL   |NULL     |RefreshDedupedError|NULL                                              |\n",
      "|rescm_dev_test              |JDE_BIMIN05              |DataPipeline|Manual     |2025-12-02 22:47:26.490677|2025-12-02 22:47:58.983915|32.0        |NULL   |NULL     |Failed             |Operation on target JDE_BI_OUT failed: Dataflow re|\n",
      "|rescm_dev_test              |JDE_BI_OUT               |Dataflow    |Manual     |2025-12-02 22:37:56.456618|2025-12-02 22:37:58.563333|2.0         |NULL   |NULL     |RefreshDedupedError|NULL                                              |\n",
      "|rescm_dev_test              |JDE_BIMIN05              |DataPipeline|Manual     |2025-12-02 22:37:39.746752|2025-12-02 22:38:14.878994|35.0        |NULL   |NULL     |Failed             |Operation on target JDE_BI_OUT failed: Dataflow re|\n",
      "|rescm_dev_test              |JDE_BI_OUT               |Dataflow    |Manual     |2025-12-02 22:33:13.419621|2025-12-02 22:33:16.166666|3.0         |NULL   |NULL     |RefreshDedupedError|NULL                                              |\n",
      "|rescm_dev_test              |JDE_BIMIN05              |DataPipeline|Manual     |2025-12-02 22:32:57.071153|2025-12-02 22:33:33.028626|36.0        |NULL   |NULL     |Failed             |Operation on target JDE_BI_OUT failed: Dataflow re|\n",
      "|rescm_dev_test              |JDE_BI_OUT               |Dataflow    |Manual     |2025-12-02 22:27:49.406769|2025-12-02 22:39:08.97254 |679.0       |NULL   |NULL     |EntityUserFailure  |Something went wrong, please try again later. If t|\n",
      "|rescm_dev_test              |JDE_BIMIN05              |DataPipeline|Manual     |2025-12-02 22:27:19.076024|2025-12-02 22:39:25.421587|726.0       |NULL   |NULL     |Failed             |Operation on target JDE_BI_OUT failed: Dataflow re|\n",
      "|rescm_dev_test              |JDE_BI_OUT               |Dataflow    |Manual     |2025-12-02 22:22:38.837069|2025-12-02 22:22:41.12    |3.0         |NULL   |NULL     |RefreshDedupedError|NULL                                              |\n",
      "|rescm_dev_test              |JDE_BIMIN05              |DataPipeline|Manual     |2025-12-02 22:22:22.79368 |2025-12-02 22:22:56.601392|34.0        |NULL   |NULL     |Failed             |Operation on target JDE_BI_OUT failed: Dataflow re|\n",
      "|rescm_dev_test              |JDE_BI_OUT               |Dataflow    |Manual     |2025-12-02 22:18:08.716834|2025-12-02 22:18:11.186666|3.0         |NULL   |NULL     |RefreshDedupedError|NULL                                              |\n",
      "|rescm_dev_test              |JDE_BIMIN05              |DataPipeline|Manual     |2025-12-02 22:17:46.732283|2025-12-02 22:18:27.074131|41.0        |NULL   |NULL     |Failed             |Operation on target JDE_BI_OUT failed: Dataflow re|\n",
      "|rescm_dev_test              |JDE_BI_OUT               |Dataflow    |Manual     |2025-12-02 22:08:31.240955|2025-12-02 22:08:33.66    |2.0         |NULL   |NULL     |RefreshDedupedError|NULL                                              |\n",
      "|rescm_dev_test              |JDE_BIMIN05              |DataPipeline|Manual     |2025-12-02 22:08:16.31047 |2025-12-02 22:08:49.709534|33.0        |NULL   |NULL     |Failed             |Operation on target JDE_BI_OUT failed: Dataflow re|\n",
      "|ABBA Lakehouse              |010_TreasuryDeltaExtracts|DataPipeline|Scheduled  |2025-12-02 22:00:04.515146|2025-12-02 22:00:15.083739|11.0        |NULL   |NULL     |Failed             |Operation on target GetData failed: BadRequest Err|\n",
      "+----------------------------+-------------------------+------------+-----------+--------------------------+--------------------------+------------+-------+---------+-------------------+--------------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "=== Recent Failures (Verified with User Info) ===\n",
      "+--------------------------------------------+---------------------------+--------------------------+------------------+--------------------------------------------+\n",
      "|Workspace                                   |Item Name                  |Start Time                |User Name         |Error Code                                  |\n",
      "+--------------------------------------------+---------------------------+--------------------------+------------------+--------------------------------------------+\n",
      "|ABBA Lakehouse                              |Orchestrate - Fusion MASTER|2025-11-30 23:50:00.643333|Matt Bailey       |AzureActiveDirectoryClientUserErrorException|\n",
      "|RE Sales & Marketing - Lead Management [DEV]|STUB Pipeline              |2025-11-30 23:00:02.53    |Matt Bailey       |AzureActiveDirectoryClientUserErrorException|\n",
      "|RE Service - Data Operations                |MIf_Analytics_Contracts    |2025-11-30 23:00:01.99992 |Michele Illuminati|Failed                                      |\n",
      "|ABBA Lakehouse                              |Orchestrate - Fusion MASTER|2025-11-30 22:50:00.756666|Matt Bailey       |AzureActiveDirectoryClientUserErrorException|\n",
      "|RE Sales & Marketing - Lead Management [DEV]|STUB Pipeline              |2025-11-30 22:01:00.71    |Matt Bailey       |AzureActiveDirectoryClientUserErrorException|\n",
      "|ABBA Lakehouse                              |Orchestrate - Fusion MASTER|2025-11-30 21:50:00.68    |Matt Bailey       |AzureActiveDirectoryClientUserErrorException|\n",
      "|RE Sales & Marketing - Lead Management [DEV]|STUB Pipeline              |2025-11-30 21:01:01.29    |Matt Bailey       |AzureActiveDirectoryClientUserErrorException|\n",
      "|ABBA Lakehouse                              |Orchestrate - Fusion MASTER|2025-11-30 20:50:01.203333|Matt Bailey       |AzureActiveDirectoryClientUserErrorException|\n",
      "|RE Sales & Marketing - Lead Management [DEV]|STUB Pipeline              |2025-11-30 20:00:01.103333|Matt Bailey       |AzureActiveDirectoryClientUserErrorException|\n",
      "|ABBA Lakehouse                              |Orchestrate - Fusion MASTER|2025-11-30 19:50:00.89    |Matt Bailey       |AzureActiveDirectoryClientUserErrorException|\n",
      "+--------------------------------------------+---------------------------+--------------------------+------------------+--------------------------------------------+\n",
      "\n",
      "\n",
      "=== Top Error Codes ===\n",
      "+--------------------------------------------+---------------------------+--------------------------+------------------+--------------------------------------------+\n",
      "|Workspace                                   |Item Name                  |Start Time                |User Name         |Error Code                                  |\n",
      "+--------------------------------------------+---------------------------+--------------------------+------------------+--------------------------------------------+\n",
      "|ABBA Lakehouse                              |Orchestrate - Fusion MASTER|2025-11-30 23:50:00.643333|Matt Bailey       |AzureActiveDirectoryClientUserErrorException|\n",
      "|RE Sales & Marketing - Lead Management [DEV]|STUB Pipeline              |2025-11-30 23:00:02.53    |Matt Bailey       |AzureActiveDirectoryClientUserErrorException|\n",
      "|RE Service - Data Operations                |MIf_Analytics_Contracts    |2025-11-30 23:00:01.99992 |Michele Illuminati|Failed                                      |\n",
      "|ABBA Lakehouse                              |Orchestrate - Fusion MASTER|2025-11-30 22:50:00.756666|Matt Bailey       |AzureActiveDirectoryClientUserErrorException|\n",
      "|RE Sales & Marketing - Lead Management [DEV]|STUB Pipeline              |2025-11-30 22:01:00.71    |Matt Bailey       |AzureActiveDirectoryClientUserErrorException|\n",
      "|ABBA Lakehouse                              |Orchestrate - Fusion MASTER|2025-11-30 21:50:00.68    |Matt Bailey       |AzureActiveDirectoryClientUserErrorException|\n",
      "|RE Sales & Marketing - Lead Management [DEV]|STUB Pipeline              |2025-11-30 21:01:01.29    |Matt Bailey       |AzureActiveDirectoryClientUserErrorException|\n",
      "|ABBA Lakehouse                              |Orchestrate - Fusion MASTER|2025-11-30 20:50:01.203333|Matt Bailey       |AzureActiveDirectoryClientUserErrorException|\n",
      "|RE Sales & Marketing - Lead Management [DEV]|STUB Pipeline              |2025-11-30 20:00:01.103333|Matt Bailey       |AzureActiveDirectoryClientUserErrorException|\n",
      "|ABBA Lakehouse                              |Orchestrate - Fusion MASTER|2025-11-30 19:50:00.89    |Matt Bailey       |AzureActiveDirectoryClientUserErrorException|\n",
      "+--------------------------------------------+---------------------------+--------------------------+------------------+--------------------------------------------+\n",
      "\n",
      "\n",
      "=== Top Error Codes ===\n",
      "+--------------------------------------------+-----+\n",
      "|Error Code                                  |Count|\n",
      "+--------------------------------------------+-----+\n",
      "|Failed                                      |850  |\n",
      "|JobInstanceStatusFailed                     |184  |\n",
      "|AzureActiveDirectoryClientUserErrorException|60   |\n",
      "|EntityUserFailure                           |33   |\n",
      "|RequestExecutionFailed                      |30   |\n",
      "|RefreshDedupedError                         |23   |\n",
      "|ActionUserFailure                           |9    |\n",
      "|DataflowNeverPublishedError                 |5    |\n",
      "|EvaluationGatewayUnreachable                |4    |\n",
      "|NotFound                                    |2    |\n",
      "|JobInstanceStatusNotFound                   |1    |\n",
      "|NameError                                   |1    |\n",
      "|RequestTimeout                              |1    |\n",
      "+--------------------------------------------+-----+\n",
      "\n",
      "\n",
      "=== Failures by Workspace ===\n",
      "+--------------------------------------------------+-----+\n",
      "|Workspace                                         |Count|\n",
      "+--------------------------------------------------+-----+\n",
      "|ABBA Human Resources                              |281  |\n",
      "|EDP HR Ingestion [DEV]                            |240  |\n",
      "|ABBA Lakehouse                                    |102  |\n",
      "|rescm_dev_test                                    |65   |\n",
      "|RE Service - Data Operations                      |61   |\n",
      "|RE Service - Automated Customer Onboarding Emails |56   |\n",
      "|EDP Ingestion [DEV]                               |56   |\n",
      "|EDP Monitoring - Admin                            |44   |\n",
      "|RGS - Fabric Workspace                            |34   |\n",
      "|EDP Lakehouse [DEV]                               |32   |\n",
      "|EDP Lakehouse [Test]                              |32   |\n",
      "|RE Service - Excess Toner Usage [OLD]             |31   |\n",
      "|ABBA IT Admin Workspace                           |31   |\n",
      "|ABBA Lakehouse [PRJ UAT]                          |31   |\n",
      "|RE Service - Data Hub                             |31   |\n",
      "|RE Finance - Hyperion                             |22   |\n",
      "|RE Sales & Marketing - Lead Management [DEV]      |14   |\n",
      "|Digital Academy - Data & AI Training - Teams D,E,F|6    |\n",
      "|RE IT - RPA [Sandbox]                             |6    |\n",
      "|RGS - Global Pricing                              |5    |\n",
      "+--------------------------------------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n",
      "=== Failures by User ===\n",
      "+--------------------------------------------+-----+\n",
      "|Error Code                                  |Count|\n",
      "+--------------------------------------------+-----+\n",
      "|Failed                                      |850  |\n",
      "|JobInstanceStatusFailed                     |184  |\n",
      "|AzureActiveDirectoryClientUserErrorException|60   |\n",
      "|EntityUserFailure                           |33   |\n",
      "|RequestExecutionFailed                      |30   |\n",
      "|RefreshDedupedError                         |23   |\n",
      "|ActionUserFailure                           |9    |\n",
      "|DataflowNeverPublishedError                 |5    |\n",
      "|EvaluationGatewayUnreachable                |4    |\n",
      "|NotFound                                    |2    |\n",
      "|JobInstanceStatusNotFound                   |1    |\n",
      "|NameError                                   |1    |\n",
      "|RequestTimeout                              |1    |\n",
      "+--------------------------------------------+-----+\n",
      "\n",
      "\n",
      "=== Failures by Workspace ===\n",
      "+--------------------------------------------------+-----+\n",
      "|Workspace                                         |Count|\n",
      "+--------------------------------------------------+-----+\n",
      "|ABBA Human Resources                              |281  |\n",
      "|EDP HR Ingestion [DEV]                            |240  |\n",
      "|ABBA Lakehouse                                    |102  |\n",
      "|rescm_dev_test                                    |65   |\n",
      "|RE Service - Data Operations                      |61   |\n",
      "|RE Service - Automated Customer Onboarding Emails |56   |\n",
      "|EDP Ingestion [DEV]                               |56   |\n",
      "|EDP Monitoring - Admin                            |44   |\n",
      "|RGS - Fabric Workspace                            |34   |\n",
      "|EDP Lakehouse [DEV]                               |32   |\n",
      "|EDP Lakehouse [Test]                              |32   |\n",
      "|RE Service - Excess Toner Usage [OLD]             |31   |\n",
      "|ABBA IT Admin Workspace                           |31   |\n",
      "|ABBA Lakehouse [PRJ UAT]                          |31   |\n",
      "|RE Service - Data Hub                             |31   |\n",
      "|RE Finance - Hyperion                             |22   |\n",
      "|RE Sales & Marketing - Lead Management [DEV]      |14   |\n",
      "|Digital Academy - Data & AI Training - Teams D,E,F|6    |\n",
      "|RE IT - RPA [Sandbox]                             |6    |\n",
      "|RGS - Global Pricing                              |5    |\n",
      "+--------------------------------------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n",
      "=== Failures by User ===\n",
      "+------------------+-----+\n",
      "|User Name         |Count|\n",
      "+------------------+-----+\n",
      "|NULL              |271  |\n",
      "|Jaime Melero      |209  |\n",
      "|Archana Lal       |144  |\n",
      "|Matt Bailey       |126  |\n",
      "|Elizabeth_francis |83   |\n",
      "|Gayatri Beldar    |49   |\n",
      "|Sanmi Ibitoye     |38   |\n",
      "|Michele Illuminati|30   |\n",
      "|Matteo Punzina    |30   |\n",
      "|Steven Morris     |27   |\n",
      "|Chitra Gurung     |27   |\n",
      "|Alex Caldicott    |27   |\n",
      "|Adam Zdancewicz   |24   |\n",
      "|Vamsi Madhav      |22   |\n",
      "|Rosita Stroppa    |18   |\n",
      "|Katarzyna Raciecka|17   |\n",
      "|Matthew Layman    |11   |\n",
      "|Adedamola Yusuf   |9    |\n",
      "|Amala Pai         |8    |\n",
      "|Anthony Chapman   |6    |\n",
      "+------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n",
      "=== Duration Statistics (Failed Runs) ===\n",
      "+------------------+------------+------------+\n",
      "|      Avg Duration|Max Duration|Min Duration|\n",
      "+------------------+------------+------------+\n",
      "|420.68661679135494|    128589.0|         0.0|\n",
      "+------------------+------------+------------+\n",
      "\n",
      "+------------------+-----+\n",
      "|User Name         |Count|\n",
      "+------------------+-----+\n",
      "|NULL              |271  |\n",
      "|Jaime Melero      |209  |\n",
      "|Archana Lal       |144  |\n",
      "|Matt Bailey       |126  |\n",
      "|Elizabeth_francis |83   |\n",
      "|Gayatri Beldar    |49   |\n",
      "|Sanmi Ibitoye     |38   |\n",
      "|Michele Illuminati|30   |\n",
      "|Matteo Punzina    |30   |\n",
      "|Steven Morris     |27   |\n",
      "|Chitra Gurung     |27   |\n",
      "|Alex Caldicott    |27   |\n",
      "|Adam Zdancewicz   |24   |\n",
      "|Vamsi Madhav      |22   |\n",
      "|Rosita Stroppa    |18   |\n",
      "|Katarzyna Raciecka|17   |\n",
      "|Matthew Layman    |11   |\n",
      "|Adedamola Yusuf   |9    |\n",
      "|Amala Pai         |8    |\n",
      "|Anthony Chapman   |6    |\n",
      "+------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n",
      "=== Duration Statistics (Failed Runs) ===\n",
      "+------------------+------------+------------+\n",
      "|      Avg Duration|Max Duration|Min Duration|\n",
      "+------------------+------------+------------+\n",
      "|420.68661679135494|    128589.0|         0.0|\n",
      "+------------------+------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 4. Analysis & Display\n",
    "from pyspark.sql.functions import avg, max, min, substring, count, desc, col\n",
    "\n",
    "if final_df:\n",
    "    print(\"=== Data Coverage Diagnostics ===\")\n",
    "    # Check date ranges to explain missing User IDs\n",
    "    stats = final_df.select(\n",
    "        min(\"Start Time\").alias(\"min_time\"),\n",
    "        max(\"Start Time\").alias(\"max_time\"),\n",
    "        count(\"*\").alias(\"total_count\"),\n",
    "        count(\"User ID\").alias(\"user_count\")\n",
    "    ).collect()[0]\n",
    "    \n",
    "    print(f\"Data Range: {stats['min_time']} to {stats['max_time']}\")\n",
    "    print(f\"Records with User Info: {stats['user_count']} / {stats['total_count']}\")\n",
    "    \n",
    "    if stats['user_count'] < stats['total_count']:\n",
    "        # Check the latest date that HAS user info\n",
    "        latest_user_date = final_df.filter(col(\"User ID\").isNotNull()).agg(max(\"Start Time\")).collect()[0][0]\n",
    "        print(f\"Latest Record with User Info: {latest_user_date}\")\n",
    "        if latest_user_date and stats['max_time'] > latest_user_date:\n",
    "             print(f\"‚ö†Ô∏è WARNING: No User Info available for failures after {latest_user_date}. Audit Logs may be missing for recent dates.\")\n",
    "\n",
    "    print(\"\\n=== Recent Failures (Detailed) ===\")\n",
    "    # Added Invoke Type and truncated Error Message\n",
    "    final_df.orderBy(col(\"Start Time\").desc()).limit(60) \\\n",
    "        .select(\n",
    "            \"Workspace\", \"Item Name\", \"Item Type\", \"Invoke Type\", \n",
    "            \"Start Time\", \"End Time\", \"Duration (s)\", \"User ID\", \"User Name\", \"Error Code\", \n",
    "            substring(\"Error Message\", 1, 50).alias(\"Error Msg (Trunc)\")\n",
    "        ) \\\n",
    "        .show(truncate=False)\n",
    "        \n",
    "    # Show verified matches if we have partial data\n",
    "    if stats['user_count'] > 0 and stats['user_count'] < stats['total_count']:\n",
    "        print(\"\\n=== Recent Failures (Verified with User Info) ===\")\n",
    "        final_df.filter(col(\"User ID\").isNotNull()) \\\n",
    "            .orderBy(col(\"Start Time\").desc()).limit(10) \\\n",
    "            .select(\n",
    "                \"Workspace\", \"Item Name\", \"Start Time\", \"User Name\", \"Error Code\"\n",
    "            ) \\\n",
    "            .show(truncate=False)\n",
    "    \n",
    "    print(\"\\n=== Top Error Codes ===\")\n",
    "    error_counts = final_df.groupBy(\"Error Code\").agg(count(\"*\").alias(\"Count\")).orderBy(desc(\"Count\"))\n",
    "    error_counts.show(truncate=False)\n",
    "\n",
    "    print(\"\\n=== Failures by Workspace ===\")\n",
    "    workspace_counts = final_df.groupBy(\"Workspace\").agg(count(\"*\").alias(\"Count\")).orderBy(desc(\"Count\"))\n",
    "    workspace_counts.show(truncate=False)\n",
    "    \n",
    "    print(\"\\n=== Failures by User ===\")\n",
    "    user_counts = final_df.groupBy(\"User Name\").agg(count(\"*\").alias(\"Count\")).orderBy(desc(\"Count\"))\n",
    "    user_counts.show(truncate=False)\n",
    "\n",
    "    print(\"\\n=== Duration Statistics (Failed Runs) ===\")\n",
    "    final_df.select(\n",
    "        avg(\"Duration (s)\").alias(\"Avg Duration\"),\n",
    "        max(\"Duration (s)\").alias(\"Max Duration\"),\n",
    "        min(\"Duration (s)\").alias(\"Min Duration\")\n",
    "    ).show()\n",
    "else:\n",
    "    print(\"No data available for analysis.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f8848816",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Recent Failures (Detailed) ===\n"
     ]
    }
   ],
   "source": [
    "# 5. show random rows up to 100 rows of the recent failures detailed\n",
    "if final_df:\n",
    "    print(\"=== Recent Failures (Detailed) ===\")\n",
    "    # Added Invoke Type and truncated Error Message\n",
    "    final_df.orderBy(col(\"Start Time\").desc()).limit(100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fe7278c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/12/03 01:54:28 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+--------------------+------------+-----------+------------------+--------------------+---------------+-----------------+--------------------+\n",
      "|summary|           Workspace|           Item Name|   Item Type|Invoke Type|      Duration (s)|             User ID|      User Name|       Error Code|       Error Message|\n",
      "+-------+--------------------+--------------------+------------+-----------+------------------+--------------------+---------------+-----------------+--------------------+\n",
      "|  count|                1203|                1203|        1203|       1203|              1203|                 932|            932|             1203|                1175|\n",
      "|   mean|                NULL|                NULL|        NULL|       NULL|420.68661679135494|                NULL|           NULL|             NULL|                NULL|\n",
      "| stddev|                NULL|                NULL|        NULL|       NULL| 3976.839741573941|                NULL|           NULL|             NULL|                NULL|\n",
      "|    min|ABBA Human Resources|000_AD_Create_Tables|DataPipeline|     Manual|               0.0|Adam.Zdancewicz@r...|Adam Zdancewicz|ActionUserFailure|Failed to run the...|\n",
      "|    max|      rescm_dev_test|           view_test|    Notebook|  Scheduled|          128589.0|vincent.mougel@ri...| Vincent Mougel|   RequestTimeout|{\"name\":\"NameErro...|\n",
      "+-------+--------------------+--------------------+------------+-----------+------------------+--------------------+---------------+-----------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 6. describe the recent failures detailed table \n",
    "final_df.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "245242e4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fabric-monitoring",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
