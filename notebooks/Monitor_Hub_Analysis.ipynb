{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f5790268",
   "metadata": {},
   "source": [
    "# Monitor Hub Analysis\n",
    "\n",
    "This notebook provides an interactive environment to run the Monitor Hub Analysis pipeline and explore the results.\n",
    "\n",
    "## Recent Updates (v0.1.12)\n",
    "- **Smart Scope Detection**: The pipeline now attempts **Tenant-Wide** extraction first. If Admin permissions are missing, it automatically falls back to **Member-Only** scope.\n",
    "- **Parquet Support**: The pipeline exports enriched data to Parquet format, enabling faster loading and direct integration with Delta Tables.\n",
    "- **Datetime Parsing Fix**: The pipeline has been updated to robustly handle mixed timezone formats in activity logs.\n",
    "- **Pipeline Integration**: The notebook uses the updated `MonitorHubPipeline` class for end-to-end execution.\n",
    "\n",
    "## Usage\n",
    "1. Ensure your environment is activated: `conda activate fabric-monitoring`\n",
    "2. Run the cells below to execute the analysis.\n",
    "3. The pipeline will:\n",
    "    - Extract historical data (Tenant-Wide with Fallback).\n",
    "    - Enrich data with job details.\n",
    "    - Generate CSV reports in the `exports/monitor_hub_analysis` directory (or configured output)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9c1bad86",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Library found: usf_fabric_monitoring v0.1.6\n",
      "   You are using the correct version.\n"
     ]
    }
   ],
   "source": [
    "# ‚úÖ VERIFY INSTALLATION\n",
    "# Since we have uploaded the .whl to your Fabric Environment, it should be installed automatically.\n",
    "# Run this cell to confirm the correct version (v0.1.12) is loaded.\n",
    "\n",
    "import importlib.metadata\n",
    "\n",
    "try:\n",
    "    version = importlib.metadata.version(\"usf_fabric_monitoring\")\n",
    "    print(f\"‚úÖ Library found: usf_fabric_monitoring v{version}\")\n",
    "    \n",
    "    if version >= \"0.1.12\":\n",
    "        print(\"   You are using the correct version.\")\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è  WARNING: Expected v0.1.12+ but found v{version}.\")\n",
    "        print(\"   Please check your Fabric Environment settings and ensure the new wheel is published.\")\n",
    "        \n",
    "except importlib.metadata.PackageNotFoundError:\n",
    "    print(\"‚ùå Library NOT found.\")\n",
    "    print(\"   Please ensure you have attached the 'Fabric Environment' containing the .whl file to this notebook.\")\n",
    "    print(\"   Alternatively, upload the .whl file to the Lakehouse 'Files' section and pip install it from there.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bfdf136",
   "metadata": {},
   "source": [
    "# Monitor Hub Analysis Pipeline\n",
    "\n",
    "## Overview\n",
    "This notebook executes the **Monitor Hub Analysis Pipeline**, which is designed to provide deep insights into Microsoft Fabric activity. It extracts historical data, calculates key performance metrics, and generates comprehensive reports to help identify:\n",
    "- Constant failures and reliability issues.\n",
    "- Excess activity by users, locations, or domains.\n",
    "- Historical performance trends over the last 90 days.\n",
    "\n",
    "## Key Features & Recent Updates (v0.1.12)\n",
    "The pipeline has been enhanced to support enterprise-grade monitoring workflows:\n",
    "\n",
    "1.  **Smart Scope Detection (v0.1.12)**:\n",
    "    -   **Primary Strategy**: Attempts to use Power BI Admin APIs for full **Tenant-Wide** visibility.\n",
    "    -   **Automatic Fallback**: If Admin permissions are missing (401/403), it gracefully reverts to **Member-Only** mode.\n",
    "    -   **Benefit**: Ensures maximum visibility allowed by your credentials without crashing.\n",
    "\n",
    "2.  **Parquet Integration (New in v0.1.8)**:\n",
    "    -   Automatically persists merged activity data (Activities, Workspaces, Items) to Parquet format.\n",
    "    -   Enables direct integration with Delta Tables and downstream analytics (e.g., Power BI Direct Lake).\n",
    "    -   Serves as the \"Source of Truth\" for the analysis steps in this notebook.\n",
    "\n",
    "3.  **Automatic Persistence & Path Resolution**:\n",
    "    -   **Automatic Lakehouse Resolution**: Relative paths (e.g., `exports/`) are automatically mapped to `/lakehouse/default/Files/` in Fabric.\n",
    "    -   **Sequential Orchestration**: Handles the entire data lifecycle (Activity Extraction -> Job Detail Extraction -> Merging -> Analysis).\n",
    "    -   **Enhanced Reliability**: Ensures JSON exports and CSV reports are saved to persistent storage, not ephemeral nodes.\n",
    "\n",
    "## How to Use\n",
    "1. **Install Package**: The first cell installs the `usf_fabric_monitoring` package into the current session.\n",
    "2. **Configure Credentials**: Ensure your Service Principal credentials (`AZURE_CLIENT_ID`, `AZURE_CLIENT_SECRET`, `AZURE_TENANT_ID`) are available.\n",
    "3. **Set Parameters**:\n",
    "    - `DAYS_TO_ANALYZE`: Number of days of history to fetch (default: 90).\n",
    "    - `OUTPUT_DIR`: Path where reports will be saved (can now be relative!).\n",
    "4. **Run Analysis**: Execute the pipeline cell. It will:\n",
    "    - Fetch data from Fabric APIs.\n",
    "    - Process and enrich the data.\n",
    "    - Save CSV reports and Parquet files to the specified `OUTPUT_DIR`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a5f15df1",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "from usf_fabric_monitoring.core.pipeline import MonitorHubPipeline\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8c0cce96",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì¶ Package Location: /home/sanmi/miniconda3/envs/fabric-monitoring/lib/python3.11/site-packages/usf_fabric_monitoring\n",
      "‚ùå WARNING: You are still running the OLD code.\n",
      "   üëâ ACTION: Restart the kernel and run the install cell above again.\n"
     ]
    }
   ],
   "source": [
    "import inspect\n",
    "import os\n",
    "import usf_fabric_monitoring\n",
    "from usf_fabric_monitoring.core.pipeline import MonitorHubPipeline\n",
    "\n",
    "print(f\"üì¶ Package Location: {os.path.dirname(usf_fabric_monitoring.__file__)}\")\n",
    "\n",
    "# Verify we are running the NEW code (v0.1.12)\n",
    "try:\n",
    "    # Check for the new _save_to_parquet method in pipeline which indicates v0.1.8+\n",
    "    src = inspect.getsource(MonitorHubPipeline)\n",
    "    if \"_save_to_parquet\" in src:\n",
    "        print(\"‚úÖ SUCCESS: You are running the updated code (v0.1.12).\")\n",
    "        print(\"   Feature Verified: Parquet Integration & Smart Scope Detection\")\n",
    "    else:\n",
    "        print(\"‚ùå WARNING: You are still running the OLD code.\")\n",
    "        print(\"   üëâ ACTION: Restart the kernel and run the install cell above again.\")\n",
    "except AttributeError:\n",
    "    print(\"‚ùå WARNING: Could not inspect source code. You might be running an optimized .pyc version.\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Could not verify source code: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a349952b",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: No .env file found at /lakehouse/default/Files/dot_env_files/.env or .env\n",
      "\n",
      "üîê IDENTITY CHECK:\n",
      "‚úÖ Service Principal Configured\n",
      "   Client ID: 4a49...64f9\n",
      "   Tenant ID: dd29478d-624e-429e-b453-fffc969ac768\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# --- CREDENTIAL MANAGEMENT ---\n",
    "\n",
    "# Option 1: Load from .env file (Lakehouse or Local)\n",
    "# We check the Lakehouse path first, then fallback to local .env\n",
    "LAKEHOUSE_ENV_PATH = \"/lakehouse/default/Files/dot_env_files/.env\"\n",
    "LOCAL_ENV_PATH = \".env\"\n",
    "\n",
    "# Force override=True to ensure we pick up changes to the file even if env vars are already set\n",
    "if os.path.exists(LAKEHOUSE_ENV_PATH):\n",
    "    print(f\"Loading configuration from Lakehouse: {LAKEHOUSE_ENV_PATH}\")\n",
    "    load_dotenv(LAKEHOUSE_ENV_PATH, override=True)\n",
    "elif os.path.exists(LOCAL_ENV_PATH):\n",
    "    print(f\"Loading configuration from Local: {os.path.abspath(LOCAL_ENV_PATH)}\")\n",
    "    load_dotenv(LOCAL_ENV_PATH, override=True)\n",
    "else:\n",
    "    print(f\"Warning: No .env file found at {LAKEHOUSE_ENV_PATH} or {LOCAL_ENV_PATH}\")\n",
    "\n",
    "# Option 2: Load from Azure Key Vault (Best Practice)\n",
    "# Uncomment and configure this section to use Azure Key Vault\n",
    "# try:\n",
    "#     from notebookutils import mssparkutils\n",
    "#     KEY_VAULT_NAME = \"YourKeyVaultName\"\n",
    "#     os.environ[\"AZURE_CLIENT_ID\"] = mssparkutils.credentials.getSecret(KEY_VAULT_NAME, \"Fabric-Client-ID\")\n",
    "#     os.environ[\"AZURE_CLIENT_SECRET\"] = mssparkutils.credentials.getSecret(KEY_VAULT_NAME, \"Fabric-Client-Secret\")\n",
    "#     os.environ[\"AZURE_TENANT_ID\"] = mssparkutils.credentials.getSecret(KEY_VAULT_NAME, \"Fabric-Tenant-ID\")\n",
    "# except ImportError:\n",
    "#     pass # Not running in Fabric or notebookutils not available\n",
    "# except Exception as e:\n",
    "#     print(f\"Key Vault access failed: {e}\")\n",
    "\n",
    "# Verify credentials are present\n",
    "required_vars = [\"AZURE_CLIENT_ID\", \"AZURE_CLIENT_SECRET\", \"AZURE_TENANT_ID\"]\n",
    "missing = [v for v in required_vars if not os.getenv(v)]\n",
    "\n",
    "print(\"\\nüîê IDENTITY CHECK:\")\n",
    "if missing:\n",
    "    print(f\"‚ùå Missing required environment variables: {', '.join(missing)}\")\n",
    "    print(\"   ‚ö†Ô∏è  System will fallback to DefaultAzureCredential (User Identity or Managed Identity)\")\n",
    "else:\n",
    "    client_id = os.getenv(\"AZURE_CLIENT_ID\")\n",
    "    masked_id = f\"{client_id[:4]}...{client_id[-4:]}\" if client_id and len(client_id) > 8 else \"********\"\n",
    "    print(f\"‚úÖ Service Principal Configured\")\n",
    "    print(f\"   Client ID: {masked_id}\")\n",
    "    print(f\"   Tenant ID: {os.getenv('AZURE_TENANT_ID')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ae37982a",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "# Configuration\n",
    "DAYS_TO_ANALYZE = 28\n",
    "\n",
    "# OUTPUT_DIR: Where to save the reports.\n",
    "# v0.1.6+ Update: You can now provide a relative path (e.g., \"monitor_hub_analysis\") \n",
    "# and it will automatically resolve to \"/lakehouse/default/Files/monitor_hub_analysis\" \n",
    "# when running in Fabric.\n",
    "OUTPUT_DIR = \"monitor_hub_analysis\" \n",
    "\n",
    "# If you prefer an explicit absolute path, you can still use it:\n",
    "# OUTPUT_DIR = \"/lakehouse/default/Files/monitor_hub_analysis\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2a4b70a5",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-12-03 23:51:36 | INFO | usf_fabric_monitoring | Monitor Hub Pipeline initialized\n",
      "2025-12-03 23:51:36 | INFO | usf_fabric_monitoring | Starting Monitor Hub analysis for 28 days (API max 28)\n",
      "2025-12-03 23:51:36 | INFO | usf_fabric_monitoring | Step 1: Extracting historical activities from Fabric APIs\n",
      "2025-12-03 23:51:36 | INFO | usf_fabric_monitoring.scripts.extract_historical_data | üîê Authenticating with Microsoft Fabric...\n",
      "2025-12-03 23:51:36 | INFO | usf_fabric_monitoring.core.auth | Using Service Principal credentials\n",
      "2025-12-03 23:51:36 | INFO | usf_fabric_monitoring.scripts.extract_historical_data | üì° Initializing Fabric data extractor...\n",
      "2025-12-03 23:51:36 | INFO | usf_fabric_monitoring.scripts.extract_historical_data | üß™ Testing API connectivity...\n",
      "2025-12-03 23:51:36 | INFO | usf_fabric_monitoring.core.auth | Acquiring Fabric API access token via Azure Identity\n",
      "2025-12-03 23:51:36 | INFO | usf_fabric_monitoring | Starting Monitor Hub analysis for 28 days (API max 28)\n",
      "2025-12-03 23:51:36 | INFO | usf_fabric_monitoring | Step 1: Extracting historical activities from Fabric APIs\n",
      "2025-12-03 23:51:36 | INFO | usf_fabric_monitoring.scripts.extract_historical_data | üîê Authenticating with Microsoft Fabric...\n",
      "2025-12-03 23:51:36 | INFO | usf_fabric_monitoring.core.auth | Using Service Principal credentials\n",
      "2025-12-03 23:51:36 | INFO | usf_fabric_monitoring.scripts.extract_historical_data | üì° Initializing Fabric data extractor...\n",
      "2025-12-03 23:51:36 | INFO | usf_fabric_monitoring.scripts.extract_historical_data | üß™ Testing API connectivity...\n",
      "2025-12-03 23:51:36 | INFO | usf_fabric_monitoring.core.auth | Acquiring Fabric API access token via Azure Identity\n",
      "2025-12-03 23:51:36 | INFO | usf_fabric_monitoring.core.auth | Fabric token acquired, expires at: 2025-12-04 00:51:35\n",
      "2025-12-03 23:51:36 | INFO | usf_fabric_monitoring.core.auth | Acquiring Power BI API access token\n",
      "2025-12-03 23:51:36 | INFO | usf_fabric_monitoring.core.auth | Fabric token acquired, expires at: 2025-12-04 00:51:35\n",
      "2025-12-03 23:51:36 | INFO | usf_fabric_monitoring.core.auth | Acquiring Power BI API access token\n",
      "2025-12-03 23:51:36 | INFO | usf_fabric_monitoring.core.auth | Power BI token acquired, expires at: 2025-12-04 00:51:35\n",
      "2025-12-03 23:51:36 | INFO | usf_fabric_monitoring.core.auth | Service principal credentials validated successfully\n",
      "2025-12-03 23:51:36 | INFO | usf_fabric_monitoring.core.extractor | Fetching member workspaces (legacy behavior)\n",
      "2025-12-03 23:51:36 | INFO | usf_fabric_monitoring.core.auth | Power BI token acquired, expires at: 2025-12-04 00:51:35\n",
      "2025-12-03 23:51:36 | INFO | usf_fabric_monitoring.core.auth | Service principal credentials validated successfully\n",
      "2025-12-03 23:51:36 | INFO | usf_fabric_monitoring.core.extractor | Fetching member workspaces (legacy behavior)\n",
      "2025-12-03 23:51:36 | INFO | usf_fabric_monitoring.core.extractor | Retrieved 172 member workspaces\n",
      "2025-12-03 23:51:36 | INFO | usf_fabric_monitoring.core.extractor | Retrieved 172 member workspaces\n",
      "2025-12-03 23:51:37 | INFO | usf_fabric_monitoring.scripts.extract_historical_data | üì• Extracting activities for 28 days...\n",
      "2025-12-03 23:51:37 | INFO | usf_fabric_monitoring.scripts.extract_historical_data |   Processing 2025-11-05...\n",
      "2025-12-03 23:51:37 | INFO | usf_fabric_monitoring.core.extractor | Using tenant-wide Power BI Admin API for 2025-11-05\n",
      "2025-12-03 23:51:37 | INFO | usf_fabric_monitoring.core.extractor | Fetching tenant-wide activities from 2025-11-05 00:00:00 to 2025-11-05 23:59:59\n",
      "2025-12-03 23:51:37 | INFO | usf_fabric_monitoring.scripts.extract_historical_data | üì• Extracting activities for 28 days...\n",
      "2025-12-03 23:51:37 | INFO | usf_fabric_monitoring.scripts.extract_historical_data |   Processing 2025-11-05...\n",
      "2025-12-03 23:51:37 | INFO | usf_fabric_monitoring.core.extractor | Using tenant-wide Power BI Admin API for 2025-11-05\n",
      "2025-12-03 23:51:37 | INFO | usf_fabric_monitoring.core.extractor | Fetching tenant-wide activities from 2025-11-05 00:00:00 to 2025-11-05 23:59:59\n",
      "2025-12-03 23:51:37 | ERROR | usf_fabric_monitoring.core.extractor | Failed to fetch tenant-wide activities: 400 Client Error: Bad Request for url: https://api.powerbi.com/v1.0/myorg/admin/activityevents?startDateTime='2025-11-05T00:00:00'&endDateTime='2025-11-05T23:59:59'\n",
      "2025-12-03 23:51:37 | ERROR | usf_fabric_monitoring.scripts.extract_historical_data |   ‚úó 2025-11-05: Failed - 400 Client Error: Bad Request for url: https://api.powerbi.com/v1.0/myorg/admin/activityevents?startDateTime='2025-11-05T00:00:00'&endDateTime='2025-11-05T23:59:59'\n",
      "  ‚úó 2025-11-05: Failed - 400 Client Error: Bad Request for url: https://api.powerbi.com/v1.0/myorg/admin/activityevents?startDateTime='2025-11-05T00:00:00'&endDateTime='2025-11-05T23:59:59'\n",
      "2025-12-03 23:51:37 | INFO | usf_fabric_monitoring.scripts.extract_historical_data |   Processing 2025-11-06...\n",
      "2025-12-03 23:51:37 | INFO | usf_fabric_monitoring.scripts.extract_historical_data |   ‚úì 2025-11-06: Found existing local file (Skipping API)\n",
      "  ‚úì 2025-11-06: Found existing local file (Skipping API)\n",
      "2025-12-03 23:51:37 | INFO | usf_fabric_monitoring.scripts.extract_historical_data |   Processing 2025-11-07...\n",
      "2025-12-03 23:51:37 | INFO | usf_fabric_monitoring.scripts.extract_historical_data |   ‚úì 2025-11-07: Found existing local file (Skipping API)\n",
      "  ‚úì 2025-11-07: Found existing local file (Skipping API)\n",
      "2025-12-03 23:51:37 | INFO | usf_fabric_monitoring.scripts.extract_historical_data |   Processing 2025-11-08...\n",
      "2025-12-03 23:51:37 | INFO | usf_fabric_monitoring.scripts.extract_historical_data |   ‚úì 2025-11-08: Found existing local file (Skipping API)\n",
      "  ‚úì 2025-11-08: Found existing local file (Skipping API)\n",
      "2025-12-03 23:51:37 | INFO | usf_fabric_monitoring.scripts.extract_historical_data |   Processing 2025-11-09...\n",
      "2025-12-03 23:51:37 | INFO | usf_fabric_monitoring.scripts.extract_historical_data |   ‚úì 2025-11-09: Found existing local file (Skipping API)\n",
      "  ‚úì 2025-11-09: Found existing local file (Skipping API)\n",
      "2025-12-03 23:51:37 | INFO | usf_fabric_monitoring.scripts.extract_historical_data |   Processing 2025-11-10...\n",
      "2025-12-03 23:51:37 | INFO | usf_fabric_monitoring.scripts.extract_historical_data |   ‚úì 2025-11-10: Found existing local file (Skipping API)\n",
      "  ‚úì 2025-11-10: Found existing local file (Skipping API)\n",
      "2025-12-03 23:51:37 | ERROR | usf_fabric_monitoring.core.extractor | Failed to fetch tenant-wide activities: 400 Client Error: Bad Request for url: https://api.powerbi.com/v1.0/myorg/admin/activityevents?startDateTime='2025-11-05T00:00:00'&endDateTime='2025-11-05T23:59:59'\n",
      "2025-12-03 23:51:37 | ERROR | usf_fabric_monitoring.scripts.extract_historical_data |   ‚úó 2025-11-05: Failed - 400 Client Error: Bad Request for url: https://api.powerbi.com/v1.0/myorg/admin/activityevents?startDateTime='2025-11-05T00:00:00'&endDateTime='2025-11-05T23:59:59'\n",
      "  ‚úó 2025-11-05: Failed - 400 Client Error: Bad Request for url: https://api.powerbi.com/v1.0/myorg/admin/activityevents?startDateTime='2025-11-05T00:00:00'&endDateTime='2025-11-05T23:59:59'\n",
      "2025-12-03 23:51:37 | INFO | usf_fabric_monitoring.scripts.extract_historical_data |   Processing 2025-11-06...\n",
      "2025-12-03 23:51:37 | INFO | usf_fabric_monitoring.scripts.extract_historical_data |   ‚úì 2025-11-06: Found existing local file (Skipping API)\n",
      "  ‚úì 2025-11-06: Found existing local file (Skipping API)\n",
      "2025-12-03 23:51:37 | INFO | usf_fabric_monitoring.scripts.extract_historical_data |   Processing 2025-11-07...\n",
      "2025-12-03 23:51:37 | INFO | usf_fabric_monitoring.scripts.extract_historical_data |   ‚úì 2025-11-07: Found existing local file (Skipping API)\n",
      "  ‚úì 2025-11-07: Found existing local file (Skipping API)\n",
      "2025-12-03 23:51:37 | INFO | usf_fabric_monitoring.scripts.extract_historical_data |   Processing 2025-11-08...\n",
      "2025-12-03 23:51:37 | INFO | usf_fabric_monitoring.scripts.extract_historical_data |   ‚úì 2025-11-08: Found existing local file (Skipping API)\n",
      "  ‚úì 2025-11-08: Found existing local file (Skipping API)\n",
      "2025-12-03 23:51:37 | INFO | usf_fabric_monitoring.scripts.extract_historical_data |   Processing 2025-11-09...\n",
      "2025-12-03 23:51:37 | INFO | usf_fabric_monitoring.scripts.extract_historical_data |   ‚úì 2025-11-09: Found existing local file (Skipping API)\n",
      "  ‚úì 2025-11-09: Found existing local file (Skipping API)\n",
      "2025-12-03 23:51:37 | INFO | usf_fabric_monitoring.scripts.extract_historical_data |   Processing 2025-11-10...\n",
      "2025-12-03 23:51:37 | INFO | usf_fabric_monitoring.scripts.extract_historical_data |   ‚úì 2025-11-10: Found existing local file (Skipping API)\n",
      "  ‚úì 2025-11-10: Found existing local file (Skipping API)\n",
      "2025-12-03 23:51:37 | INFO | usf_fabric_monitoring.scripts.extract_historical_data |   Processing 2025-11-11...\n",
      "2025-12-03 23:51:37 | INFO | usf_fabric_monitoring.scripts.extract_historical_data |   ‚úì 2025-11-11: Found existing local file (Skipping API)\n",
      "  ‚úì 2025-11-11: Found existing local file (Skipping API)\n",
      "2025-12-03 23:51:37 | INFO | usf_fabric_monitoring.scripts.extract_historical_data |   Processing 2025-11-11...\n",
      "2025-12-03 23:51:37 | INFO | usf_fabric_monitoring.scripts.extract_historical_data |   ‚úì 2025-11-11: Found existing local file (Skipping API)\n",
      "  ‚úì 2025-11-11: Found existing local file (Skipping API)\n",
      "2025-12-03 23:51:37 | INFO | usf_fabric_monitoring.scripts.extract_historical_data |   Processing 2025-11-12...\n",
      "2025-12-03 23:51:37 | INFO | usf_fabric_monitoring.scripts.extract_historical_data |   ‚úì 2025-11-12: Found existing local file (Skipping API)\n",
      "  ‚úì 2025-11-12: Found existing local file (Skipping API)\n",
      "2025-12-03 23:51:37 | INFO | usf_fabric_monitoring.scripts.extract_historical_data |   Processing 2025-11-13...\n",
      "2025-12-03 23:51:37 | INFO | usf_fabric_monitoring.scripts.extract_historical_data |   ‚úì 2025-11-13: Found existing local file (Skipping API)\n",
      "  ‚úì 2025-11-13: Found existing local file (Skipping API)\n",
      "2025-12-03 23:51:37 | INFO | usf_fabric_monitoring.scripts.extract_historical_data |   Processing 2025-11-14...\n",
      "2025-12-03 23:51:37 | INFO | usf_fabric_monitoring.scripts.extract_historical_data |   ‚úì 2025-11-14: Found existing local file (Skipping API)\n",
      "  ‚úì 2025-11-14: Found existing local file (Skipping API)\n",
      "2025-12-03 23:51:37 | INFO | usf_fabric_monitoring.scripts.extract_historical_data |   Processing 2025-11-15...\n",
      "2025-12-03 23:51:37 | INFO | usf_fabric_monitoring.scripts.extract_historical_data |   ‚úì 2025-11-15: Found existing local file (Skipping API)\n",
      "  ‚úì 2025-11-15: Found existing local file (Skipping API)\n",
      "2025-12-03 23:51:37 | INFO | usf_fabric_monitoring.scripts.extract_historical_data |   Processing 2025-11-16...\n",
      "2025-12-03 23:51:37 | INFO | usf_fabric_monitoring.scripts.extract_historical_data |   ‚úì 2025-11-16: Found existing local file (Skipping API)\n",
      "  ‚úì 2025-11-16: Found existing local file (Skipping API)\n",
      "2025-12-03 23:51:37 | INFO | usf_fabric_monitoring.scripts.extract_historical_data |   Processing 2025-11-17...\n",
      "2025-12-03 23:51:37 | INFO | usf_fabric_monitoring.scripts.extract_historical_data |   ‚úì 2025-11-17: Found existing local file (Skipping API)\n",
      "  ‚úì 2025-11-17: Found existing local file (Skipping API)\n",
      "2025-12-03 23:51:37 | INFO | usf_fabric_monitoring.scripts.extract_historical_data |   Processing 2025-11-18...\n",
      "2025-12-03 23:51:37 | INFO | usf_fabric_monitoring.scripts.extract_historical_data |   Processing 2025-11-12...\n",
      "2025-12-03 23:51:37 | INFO | usf_fabric_monitoring.scripts.extract_historical_data |   ‚úì 2025-11-12: Found existing local file (Skipping API)\n",
      "  ‚úì 2025-11-12: Found existing local file (Skipping API)\n",
      "2025-12-03 23:51:37 | INFO | usf_fabric_monitoring.scripts.extract_historical_data |   Processing 2025-11-13...\n",
      "2025-12-03 23:51:37 | INFO | usf_fabric_monitoring.scripts.extract_historical_data |   ‚úì 2025-11-13: Found existing local file (Skipping API)\n",
      "  ‚úì 2025-11-13: Found existing local file (Skipping API)\n",
      "2025-12-03 23:51:37 | INFO | usf_fabric_monitoring.scripts.extract_historical_data |   Processing 2025-11-14...\n",
      "2025-12-03 23:51:37 | INFO | usf_fabric_monitoring.scripts.extract_historical_data |   ‚úì 2025-11-14: Found existing local file (Skipping API)\n",
      "  ‚úì 2025-11-14: Found existing local file (Skipping API)\n",
      "2025-12-03 23:51:37 | INFO | usf_fabric_monitoring.scripts.extract_historical_data |   Processing 2025-11-15...\n",
      "2025-12-03 23:51:37 | INFO | usf_fabric_monitoring.scripts.extract_historical_data |   ‚úì 2025-11-15: Found existing local file (Skipping API)\n",
      "  ‚úì 2025-11-15: Found existing local file (Skipping API)\n",
      "2025-12-03 23:51:37 | INFO | usf_fabric_monitoring.scripts.extract_historical_data |   Processing 2025-11-16...\n",
      "2025-12-03 23:51:37 | INFO | usf_fabric_monitoring.scripts.extract_historical_data |   ‚úì 2025-11-16: Found existing local file (Skipping API)\n",
      "  ‚úì 2025-11-16: Found existing local file (Skipping API)\n",
      "2025-12-03 23:51:37 | INFO | usf_fabric_monitoring.scripts.extract_historical_data |   Processing 2025-11-17...\n",
      "2025-12-03 23:51:37 | INFO | usf_fabric_monitoring.scripts.extract_historical_data |   ‚úì 2025-11-17: Found existing local file (Skipping API)\n",
      "  ‚úì 2025-11-17: Found existing local file (Skipping API)\n",
      "2025-12-03 23:51:37 | INFO | usf_fabric_monitoring.scripts.extract_historical_data |   Processing 2025-11-18...\n",
      "2025-12-03 23:51:37 | INFO | usf_fabric_monitoring.scripts.extract_historical_data |   ‚úì 2025-11-18: Found existing local file (Skipping API)\n",
      "  ‚úì 2025-11-18: Found existing local file (Skipping API)\n",
      "2025-12-03 23:51:37 | INFO | usf_fabric_monitoring.scripts.extract_historical_data |   Processing 2025-11-19...\n",
      "2025-12-03 23:51:37 | INFO | usf_fabric_monitoring.scripts.extract_historical_data |   ‚úì 2025-11-19: Found existing local file (Skipping API)\n",
      "  ‚úì 2025-11-19: Found existing local file (Skipping API)\n",
      "2025-12-03 23:51:37 | INFO | usf_fabric_monitoring.scripts.extract_historical_data |   Processing 2025-11-20...\n",
      "2025-12-03 23:51:37 | INFO | usf_fabric_monitoring.scripts.extract_historical_data |   ‚úì 2025-11-20: Found existing local file (Skipping API)\n",
      "  ‚úì 2025-11-20: Found existing local file (Skipping API)\n",
      "2025-12-03 23:51:37 | INFO | usf_fabric_monitoring.scripts.extract_historical_data |   Processing 2025-11-21...\n",
      "2025-12-03 23:51:37 | INFO | usf_fabric_monitoring.scripts.extract_historical_data |   ‚úì 2025-11-21: Found existing local file (Skipping API)\n",
      "  ‚úì 2025-11-21: Found existing local file (Skipping API)\n",
      "2025-12-03 23:51:37 | INFO | usf_fabric_monitoring.scripts.extract_historical_data |   Processing 2025-11-22...\n",
      "2025-12-03 23:51:37 | INFO | usf_fabric_monitoring.scripts.extract_historical_data |   ‚úì 2025-11-18: Found existing local file (Skipping API)\n",
      "  ‚úì 2025-11-18: Found existing local file (Skipping API)\n",
      "2025-12-03 23:51:37 | INFO | usf_fabric_monitoring.scripts.extract_historical_data |   Processing 2025-11-19...\n",
      "2025-12-03 23:51:37 | INFO | usf_fabric_monitoring.scripts.extract_historical_data |   ‚úì 2025-11-19: Found existing local file (Skipping API)\n",
      "  ‚úì 2025-11-19: Found existing local file (Skipping API)\n",
      "2025-12-03 23:51:37 | INFO | usf_fabric_monitoring.scripts.extract_historical_data |   Processing 2025-11-20...\n",
      "2025-12-03 23:51:37 | INFO | usf_fabric_monitoring.scripts.extract_historical_data |   ‚úì 2025-11-20: Found existing local file (Skipping API)\n",
      "  ‚úì 2025-11-20: Found existing local file (Skipping API)\n",
      "2025-12-03 23:51:37 | INFO | usf_fabric_monitoring.scripts.extract_historical_data |   Processing 2025-11-21...\n",
      "2025-12-03 23:51:37 | INFO | usf_fabric_monitoring.scripts.extract_historical_data |   ‚úì 2025-11-21: Found existing local file (Skipping API)\n",
      "  ‚úì 2025-11-21: Found existing local file (Skipping API)\n",
      "2025-12-03 23:51:37 | INFO | usf_fabric_monitoring.scripts.extract_historical_data |   Processing 2025-11-22...\n",
      "2025-12-03 23:51:37 | INFO | usf_fabric_monitoring.scripts.extract_historical_data |   ‚úì 2025-11-22: Found existing local file (Skipping API)\n",
      "  ‚úì 2025-11-22: Found existing local file (Skipping API)\n",
      "2025-12-03 23:51:37 | INFO | usf_fabric_monitoring.scripts.extract_historical_data |   Processing 2025-11-23...\n",
      "2025-12-03 23:51:37 | INFO | usf_fabric_monitoring.scripts.extract_historical_data |   ‚úì 2025-11-23: Found existing local file (Skipping API)\n",
      "  ‚úì 2025-11-23: Found existing local file (Skipping API)\n",
      "2025-12-03 23:51:37 | INFO | usf_fabric_monitoring.scripts.extract_historical_data |   Processing 2025-11-24...\n",
      "2025-12-03 23:51:37 | INFO | usf_fabric_monitoring.scripts.extract_historical_data |   ‚úì 2025-11-24: Found existing local file (Skipping API)\n",
      "  ‚úì 2025-11-24: Found existing local file (Skipping API)\n",
      "2025-12-03 23:51:37 | INFO | usf_fabric_monitoring.scripts.extract_historical_data |   Processing 2025-11-25...\n",
      "2025-12-03 23:51:37 | INFO | usf_fabric_monitoring.scripts.extract_historical_data |   ‚úì 2025-11-25: Found existing local file (Skipping API)\n",
      "  ‚úì 2025-11-25: Found existing local file (Skipping API)\n",
      "2025-12-03 23:51:37 | INFO | usf_fabric_monitoring.scripts.extract_historical_data |   Processing 2025-11-26...\n",
      "2025-12-03 23:51:37 | INFO | usf_fabric_monitoring.scripts.extract_historical_data |   ‚úì 2025-11-26: Found existing local file (Skipping API)\n",
      "  ‚úì 2025-11-26: Found existing local file (Skipping API)\n",
      "2025-12-03 23:51:37 | INFO | usf_fabric_monitoring.scripts.extract_historical_data |   Processing 2025-11-27...\n",
      "2025-12-03 23:51:37 | INFO | usf_fabric_monitoring.scripts.extract_historical_data |   ‚úì 2025-11-27: Found existing local file (Skipping API)\n",
      "  ‚úì 2025-11-27: Found existing local file (Skipping API)\n",
      "2025-12-03 23:51:37 | INFO | usf_fabric_monitoring.scripts.extract_historical_data |   Processing 2025-11-28...\n",
      "2025-12-03 23:51:37 | INFO | usf_fabric_monitoring.scripts.extract_historical_data |   ‚úì 2025-11-22: Found existing local file (Skipping API)\n",
      "  ‚úì 2025-11-22: Found existing local file (Skipping API)\n",
      "2025-12-03 23:51:37 | INFO | usf_fabric_monitoring.scripts.extract_historical_data |   Processing 2025-11-23...\n",
      "2025-12-03 23:51:37 | INFO | usf_fabric_monitoring.scripts.extract_historical_data |   ‚úì 2025-11-23: Found existing local file (Skipping API)\n",
      "  ‚úì 2025-11-23: Found existing local file (Skipping API)\n",
      "2025-12-03 23:51:37 | INFO | usf_fabric_monitoring.scripts.extract_historical_data |   Processing 2025-11-24...\n",
      "2025-12-03 23:51:37 | INFO | usf_fabric_monitoring.scripts.extract_historical_data |   ‚úì 2025-11-24: Found existing local file (Skipping API)\n",
      "  ‚úì 2025-11-24: Found existing local file (Skipping API)\n",
      "2025-12-03 23:51:37 | INFO | usf_fabric_monitoring.scripts.extract_historical_data |   Processing 2025-11-25...\n",
      "2025-12-03 23:51:37 | INFO | usf_fabric_monitoring.scripts.extract_historical_data |   ‚úì 2025-11-25: Found existing local file (Skipping API)\n",
      "  ‚úì 2025-11-25: Found existing local file (Skipping API)\n",
      "2025-12-03 23:51:37 | INFO | usf_fabric_monitoring.scripts.extract_historical_data |   Processing 2025-11-26...\n",
      "2025-12-03 23:51:37 | INFO | usf_fabric_monitoring.scripts.extract_historical_data |   ‚úì 2025-11-26: Found existing local file (Skipping API)\n",
      "  ‚úì 2025-11-26: Found existing local file (Skipping API)\n",
      "2025-12-03 23:51:37 | INFO | usf_fabric_monitoring.scripts.extract_historical_data |   Processing 2025-11-27...\n",
      "2025-12-03 23:51:37 | INFO | usf_fabric_monitoring.scripts.extract_historical_data |   ‚úì 2025-11-27: Found existing local file (Skipping API)\n",
      "  ‚úì 2025-11-27: Found existing local file (Skipping API)\n",
      "2025-12-03 23:51:37 | INFO | usf_fabric_monitoring.scripts.extract_historical_data |   Processing 2025-11-28...\n",
      "2025-12-03 23:51:37 | INFO | usf_fabric_monitoring.scripts.extract_historical_data |   ‚úì 2025-11-28: Found existing local file (Skipping API)\n",
      "  ‚úì 2025-11-28: Found existing local file (Skipping API)\n",
      "2025-12-03 23:51:37 | INFO | usf_fabric_monitoring.scripts.extract_historical_data |   Processing 2025-11-29...\n",
      "2025-12-03 23:51:37 | INFO | usf_fabric_monitoring.scripts.extract_historical_data |   ‚úì 2025-11-29: Found existing local file (Skipping API)\n",
      "  ‚úì 2025-11-29: Found existing local file (Skipping API)\n",
      "2025-12-03 23:51:37 | INFO | usf_fabric_monitoring.scripts.extract_historical_data |   Processing 2025-11-30...\n",
      "2025-12-03 23:51:37 | INFO | usf_fabric_monitoring.scripts.extract_historical_data |   ‚úì 2025-11-30: Found existing local file (Skipping API)\n",
      "  ‚úì 2025-11-30: Found existing local file (Skipping API)\n",
      "2025-12-03 23:51:37 | INFO | usf_fabric_monitoring.scripts.extract_historical_data |   Processing 2025-12-01...\n",
      "2025-12-03 23:51:37 | INFO | usf_fabric_monitoring.scripts.extract_historical_data |   ‚úì 2025-12-01: Found existing local file (Skipping API)\n",
      "  ‚úì 2025-12-01: Found existing local file (Skipping API)\n",
      "2025-12-03 23:51:37 | INFO | usf_fabric_monitoring.scripts.extract_historical_data |   Processing 2025-12-02...\n",
      "2025-12-03 23:51:37 | INFO | usf_fabric_monitoring.scripts.extract_historical_data |   ‚úì 2025-12-02: Found existing local file (Skipping API)\n",
      "  ‚úì 2025-12-02: Found existing local file (Skipping API)\n",
      "2025-12-03 23:51:37 | INFO | usf_fabric_monitoring.scripts.extract_historical_data | ‚úÖ Retrieved 0 NEW activities across 28 days\n",
      "2025-12-03 23:51:37 | WARNING | usf_fabric_monitoring | ‚ö†Ô∏è  Failed to extract data for 1 days: ['2025-11-05']\n",
      "2025-12-03 23:51:37 | INFO | usf_fabric_monitoring | Step 1b: Extracting detailed job history (for accurate failure tracking)\n",
      "2025-12-03 23:51:37 | INFO | usf_fabric_monitoring.core.auth | Using Service Principal credentials\n",
      "2025-12-03 23:51:37 | INFO | usf_fabric_monitoring.scripts.extract_historical_data |   ‚úì 2025-11-28: Found existing local file (Skipping API)\n",
      "  ‚úì 2025-11-28: Found existing local file (Skipping API)\n",
      "2025-12-03 23:51:37 | INFO | usf_fabric_monitoring.scripts.extract_historical_data |   Processing 2025-11-29...\n",
      "2025-12-03 23:51:37 | INFO | usf_fabric_monitoring.scripts.extract_historical_data |   ‚úì 2025-11-29: Found existing local file (Skipping API)\n",
      "  ‚úì 2025-11-29: Found existing local file (Skipping API)\n",
      "2025-12-03 23:51:37 | INFO | usf_fabric_monitoring.scripts.extract_historical_data |   Processing 2025-11-30...\n",
      "2025-12-03 23:51:37 | INFO | usf_fabric_monitoring.scripts.extract_historical_data |   ‚úì 2025-11-30: Found existing local file (Skipping API)\n",
      "  ‚úì 2025-11-30: Found existing local file (Skipping API)\n",
      "2025-12-03 23:51:37 | INFO | usf_fabric_monitoring.scripts.extract_historical_data |   Processing 2025-12-01...\n",
      "2025-12-03 23:51:37 | INFO | usf_fabric_monitoring.scripts.extract_historical_data |   ‚úì 2025-12-01: Found existing local file (Skipping API)\n",
      "  ‚úì 2025-12-01: Found existing local file (Skipping API)\n",
      "2025-12-03 23:51:37 | INFO | usf_fabric_monitoring.scripts.extract_historical_data |   Processing 2025-12-02...\n",
      "2025-12-03 23:51:37 | INFO | usf_fabric_monitoring.scripts.extract_historical_data |   ‚úì 2025-12-02: Found existing local file (Skipping API)\n",
      "  ‚úì 2025-12-02: Found existing local file (Skipping API)\n",
      "2025-12-03 23:51:37 | INFO | usf_fabric_monitoring.scripts.extract_historical_data | ‚úÖ Retrieved 0 NEW activities across 28 days\n",
      "2025-12-03 23:51:37 | WARNING | usf_fabric_monitoring | ‚ö†Ô∏è  Failed to extract data for 1 days: ['2025-11-05']\n",
      "2025-12-03 23:51:37 | INFO | usf_fabric_monitoring | Step 1b: Extracting detailed job history (for accurate failure tracking)\n",
      "2025-12-03 23:51:37 | INFO | usf_fabric_monitoring.core.auth | Using Service Principal credentials\n",
      "2025-12-03 23:51:37 | INFO | fabric_item_details | Incremental run: Fetching jobs completed after 2025-12-03 17:40:37.777401\n",
      "2025-12-03 23:51:37 | INFO | fabric_item_details | Fetching workspaces...\n",
      "2025-12-03 23:51:37 | INFO | usf_fabric_monitoring.core.auth | Acquiring Fabric API access token via Azure Identity\n",
      "2025-12-03 23:51:37 | INFO | fabric_item_details | Incremental run: Fetching jobs completed after 2025-12-03 17:40:37.777401\n",
      "2025-12-03 23:51:37 | INFO | fabric_item_details | Fetching workspaces...\n",
      "2025-12-03 23:51:37 | INFO | usf_fabric_monitoring.core.auth | Acquiring Fabric API access token via Azure Identity\n",
      "2025-12-03 23:51:37 | INFO | usf_fabric_monitoring.core.auth | Fabric token acquired, expires at: 2025-12-04 00:51:36\n",
      "2025-12-03 23:51:37 | INFO | usf_fabric_monitoring.core.extractor | Fetching member workspaces (legacy behavior)\n",
      "2025-12-03 23:51:37 | INFO | usf_fabric_monitoring.core.auth | Fabric token acquired, expires at: 2025-12-04 00:51:36\n",
      "2025-12-03 23:51:37 | INFO | usf_fabric_monitoring.core.extractor | Fetching member workspaces (legacy behavior)\n",
      "2025-12-03 23:51:37 | INFO | usf_fabric_monitoring.core.extractor | Retrieved 172 member workspaces\n",
      "2025-12-03 23:51:37 | INFO | fabric_item_details | Found 172 workspaces to process\n",
      "2025-12-03 23:51:37 | INFO | fabric_item_details | Processing workspace: ABBA OSCAR IT Workspace [DEV] (611b43d5-9819-4df5-803a-e111c94a135b)\n",
      "2025-12-03 23:51:37 | INFO | usf_fabric_monitoring.core.extractor | Fetching items for workspace 611b43d5-9819-4df5-803a-e111c94a135b\n",
      "2025-12-03 23:51:37 | INFO | usf_fabric_monitoring.core.extractor | Retrieved 172 member workspaces\n",
      "2025-12-03 23:51:37 | INFO | fabric_item_details | Found 172 workspaces to process\n",
      "2025-12-03 23:51:37 | INFO | fabric_item_details | Processing workspace: ABBA OSCAR IT Workspace [DEV] (611b43d5-9819-4df5-803a-e111c94a135b)\n",
      "2025-12-03 23:51:37 | INFO | usf_fabric_monitoring.core.extractor | Fetching items for workspace 611b43d5-9819-4df5-803a-e111c94a135b\n",
      "2025-12-03 23:51:37 | INFO | usf_fabric_monitoring.core.extractor | Retrieved 36 items for workspace 611b43d5-9819-4df5-803a-e111c94a135b\n",
      "2025-12-03 23:51:37 | INFO | usf_fabric_monitoring.core.extractor | Retrieved 36 items for workspace 611b43d5-9819-4df5-803a-e111c94a135b\n",
      "2025-12-03 23:51:38 | ERROR | usf_fabric_monitoring.core.fabric_item_details | Failed to fetch job instances for item a1c4d647-4b31-4dd9-9ca6-218a8473e9e6: 400 Client Error: Bad Request for url: https://api.fabric.microsoft.com/v1/workspaces/611b43d5-9819-4df5-803a-e111c94a135b/items/a1c4d647-4b31-4dd9-9ca6-218a8473e9e6/jobs/instances\n",
      "2025-12-03 23:51:38 | ERROR | usf_fabric_monitoring.core.fabric_item_details | Failed to fetch job instances for item a1c4d647-4b31-4dd9-9ca6-218a8473e9e6: 400 Client Error: Bad Request for url: https://api.fabric.microsoft.com/v1/workspaces/611b43d5-9819-4df5-803a-e111c94a135b/items/a1c4d647-4b31-4dd9-9ca6-218a8473e9e6/jobs/instances\n",
      "2025-12-03 23:51:38 | ERROR | usf_fabric_monitoring.core.fabric_item_details | Failed to fetch job instances for item df0aca3a-3cd5-43b9-9e3c-375021486ad6: 400 Client Error: Bad Request for url: https://api.fabric.microsoft.com/v1/workspaces/611b43d5-9819-4df5-803a-e111c94a135b/items/df0aca3a-3cd5-43b9-9e3c-375021486ad6/jobs/instances\n",
      "2025-12-03 23:51:38 | ERROR | usf_fabric_monitoring.core.fabric_item_details | Failed to fetch job instances for item df0aca3a-3cd5-43b9-9e3c-375021486ad6: 400 Client Error: Bad Request for url: https://api.fabric.microsoft.com/v1/workspaces/611b43d5-9819-4df5-803a-e111c94a135b/items/df0aca3a-3cd5-43b9-9e3c-375021486ad6/jobs/instances\n",
      "2025-12-03 23:51:38 | ERROR | usf_fabric_monitoring.core.fabric_item_details | Failed to fetch job instances for item c130e751-efea-458d-98ac-79a5c2d0e145: 400 Client Error: Bad Request for url: https://api.fabric.microsoft.com/v1/workspaces/611b43d5-9819-4df5-803a-e111c94a135b/items/c130e751-efea-458d-98ac-79a5c2d0e145/jobs/instances\n",
      "2025-12-03 23:51:38 | ERROR | usf_fabric_monitoring.core.fabric_item_details | Failed to fetch job instances for item c130e751-efea-458d-98ac-79a5c2d0e145: 400 Client Error: Bad Request for url: https://api.fabric.microsoft.com/v1/workspaces/611b43d5-9819-4df5-803a-e111c94a135b/items/c130e751-efea-458d-98ac-79a5c2d0e145/jobs/instances\n",
      "2025-12-03 23:51:38 | WARNING | usf_fabric_monitoring.core.fabric_item_details | Job instances endpoint not found for item 6c0c29a0-0cd3-499f-b7e1-6de366d74dda\n",
      "2025-12-03 23:51:38 | WARNING | usf_fabric_monitoring.core.fabric_item_details | Job instances endpoint not found for item 6c0c29a0-0cd3-499f-b7e1-6de366d74dda\n",
      "2025-12-03 23:51:38 | WARNING | usf_fabric_monitoring.core.fabric_item_details | Job instances endpoint not found for item 57bfa820-5e4b-4c5c-8f6a-78e241fd5b70\n",
      "2025-12-03 23:51:38 | WARNING | usf_fabric_monitoring.core.fabric_item_details | Job instances endpoint not found for item 57bfa820-5e4b-4c5c-8f6a-78e241fd5b70\n",
      "2025-12-03 23:51:38 | WARNING | usf_fabric_monitoring.core.fabric_item_details | Job instances endpoint not found for item 8b45a200-03d5-46fd-a911-7970dff2e6db\n",
      "2025-12-03 23:51:38 | WARNING | usf_fabric_monitoring.core.fabric_item_details | Job instances endpoint not found for item 8b45a200-03d5-46fd-a911-7970dff2e6db\n",
      "2025-12-03 23:51:38 | WARNING | usf_fabric_monitoring.core.fabric_item_details | Job instances endpoint not found for item ccb0ff91-f2bf-430c-b2f7-e385b005bdda\n",
      "2025-12-03 23:51:38 | WARNING | usf_fabric_monitoring.core.fabric_item_details | Job instances endpoint not found for item ccb0ff91-f2bf-430c-b2f7-e385b005bdda\n",
      "2025-12-03 23:51:38 | WARNING | usf_fabric_monitoring.core.fabric_item_details | Job instances endpoint not found for item 1be9bb82-b36d-42fd-94f6-f4f3c3b596c3\n",
      "2025-12-03 23:51:38 | WARNING | usf_fabric_monitoring.core.fabric_item_details | Job instances endpoint not found for item 1be9bb82-b36d-42fd-94f6-f4f3c3b596c3\n",
      "2025-12-03 23:51:38 | WARNING | usf_fabric_monitoring.core.fabric_item_details | Job instances endpoint not found for item 680d6490-20a8-4f8a-abb6-6c1c4d0d07ce\n",
      "2025-12-03 23:51:38 | WARNING | usf_fabric_monitoring.core.fabric_item_details | Job instances endpoint not found for item 680d6490-20a8-4f8a-abb6-6c1c4d0d07ce\n",
      "2025-12-03 23:51:39 | WARNING | usf_fabric_monitoring.core.fabric_item_details | Job instances endpoint not found for item 8a1127b9-9a6c-4048-8cad-db7fd8322b03\n",
      "2025-12-03 23:51:39 | WARNING | usf_fabric_monitoring.core.fabric_item_details | Job instances endpoint not found for item 8a1127b9-9a6c-4048-8cad-db7fd8322b03\n",
      "2025-12-03 23:51:39 | WARNING | usf_fabric_monitoring.core.fabric_item_details | Job instances endpoint not found for item c02d4fb2-b9d3-4406-b3b0-97e8fc84cdcf\n",
      "2025-12-03 23:51:39 | WARNING | usf_fabric_monitoring.core.fabric_item_details | Job instances endpoint not found for item c02d4fb2-b9d3-4406-b3b0-97e8fc84cdcf\n",
      "2025-12-03 23:51:39 | WARNING | usf_fabric_monitoring.core.fabric_item_details | Job instances endpoint not found for item 460b5556-02c0-4a26-aa5c-adae69e19e15\n",
      "2025-12-03 23:51:39 | WARNING | usf_fabric_monitoring.core.fabric_item_details | Job instances endpoint not found for item 460b5556-02c0-4a26-aa5c-adae69e19e15\n",
      "2025-12-03 23:51:39 | WARNING | usf_fabric_monitoring.core.fabric_item_details | Job instances endpoint not found for item 54e7e5aa-3fb6-4e6d-95ab-a05e9f2517c8\n",
      "2025-12-03 23:51:39 | WARNING | usf_fabric_monitoring.core.fabric_item_details | Job instances endpoint not found for item 54e7e5aa-3fb6-4e6d-95ab-a05e9f2517c8\n",
      "2025-12-03 23:51:39 | WARNING | usf_fabric_monitoring.core.fabric_item_details | Job instances endpoint not found for item a4e72e7c-600d-44dc-a700-3bc5a9031519\n",
      "2025-12-03 23:51:39 | WARNING | usf_fabric_monitoring.core.fabric_item_details | Job instances endpoint not found for item a4e72e7c-600d-44dc-a700-3bc5a9031519\n",
      "2025-12-03 23:51:39 | WARNING | usf_fabric_monitoring.core.fabric_item_details | Job instances endpoint not found for item c047ae7e-60b6-4ec0-aef0-9ae2b9800102\n",
      "2025-12-03 23:51:39 | WARNING | usf_fabric_monitoring.core.fabric_item_details | Job instances endpoint not found for item c047ae7e-60b6-4ec0-aef0-9ae2b9800102\n",
      "2025-12-03 23:51:39 | WARNING | usf_fabric_monitoring.core.fabric_item_details | Job instances endpoint not found for item ec8bcd9c-5d8b-4e4a-bfab-65d38de3e5c4\n",
      "2025-12-03 23:51:39 | WARNING | usf_fabric_monitoring.core.fabric_item_details | Job instances endpoint not found for item ec8bcd9c-5d8b-4e4a-bfab-65d38de3e5c4\n",
      "2025-12-03 23:51:39 | WARNING | usf_fabric_monitoring.core.fabric_item_details | Job instances endpoint not found for item cc926ea0-13c3-4d4b-b38d-d47bd3fd7656\n",
      "2025-12-03 23:51:39 | WARNING | usf_fabric_monitoring.core.fabric_item_details | Job instances endpoint not found for item cc926ea0-13c3-4d4b-b38d-d47bd3fd7656\n",
      "2025-12-03 23:51:39 | WARNING | usf_fabric_monitoring.core.fabric_item_details | Job instances endpoint not found for item 24367afe-dc68-401b-9a47-9b220aa87de4\n",
      "2025-12-03 23:51:39 | WARNING | usf_fabric_monitoring.core.fabric_item_details | Job instances endpoint not found for item 24367afe-dc68-401b-9a47-9b220aa87de4\n",
      "2025-12-03 23:51:39 | WARNING | usf_fabric_monitoring.core.fabric_item_details | Job instances endpoint not found for item 03628656-e9fa-4b8f-98c2-3258bb98a9ae\n",
      "2025-12-03 23:51:39 | WARNING | usf_fabric_monitoring.core.fabric_item_details | Job instances endpoint not found for item 03628656-e9fa-4b8f-98c2-3258bb98a9ae\n",
      "2025-12-03 23:51:40 | WARNING | usf_fabric_monitoring.core.fabric_item_details | Job instances endpoint not found for item 2fbf08f3-ac0a-4ff2-9038-f1d51505f7f8\n",
      "2025-12-03 23:51:40 | WARNING | usf_fabric_monitoring.core.fabric_item_details | Job instances endpoint not found for item 2fbf08f3-ac0a-4ff2-9038-f1d51505f7f8\n",
      "2025-12-03 23:51:40 | WARNING | usf_fabric_monitoring.core.fabric_item_details | Job instances endpoint not found for item 43e2353d-9fe5-4a61-80af-d23a26c5388d\n",
      "2025-12-03 23:51:40 | WARNING | usf_fabric_monitoring.core.fabric_item_details | Job instances endpoint not found for item 43e2353d-9fe5-4a61-80af-d23a26c5388d\n",
      "2025-12-03 23:51:40 | WARNING | usf_fabric_monitoring.core.fabric_item_details | Job instances endpoint not found for item a05f3e4d-3f51-4aa4-b4a0-849af7305ce4\n",
      "2025-12-03 23:51:40 | WARNING | usf_fabric_monitoring.core.fabric_item_details | Job instances endpoint not found for item a05f3e4d-3f51-4aa4-b4a0-849af7305ce4\n",
      "2025-12-03 23:51:40 | WARNING | usf_fabric_monitoring.core.fabric_item_details | Job instances endpoint not found for item 105a6644-2e07-4261-a3ae-42981a6e87d7\n",
      "2025-12-03 23:51:40 | WARNING | usf_fabric_monitoring.core.fabric_item_details | Job instances endpoint not found for item 105a6644-2e07-4261-a3ae-42981a6e87d7\n",
      "2025-12-03 23:51:40 | WARNING | usf_fabric_monitoring.core.fabric_item_details | Job instances endpoint not found for item 0626e77a-26c9-4845-bc05-3f42389b5ca1\n",
      "2025-12-03 23:51:40 | WARNING | usf_fabric_monitoring.core.fabric_item_details | Job instances endpoint not found for item 0626e77a-26c9-4845-bc05-3f42389b5ca1\n",
      "2025-12-03 23:51:40 | WARNING | usf_fabric_monitoring.core.fabric_item_details | Job instances endpoint not found for item fbdd881a-7cf4-45d8-a32e-5e0290aa4a44\n",
      "2025-12-03 23:51:40 | WARNING | usf_fabric_monitoring.core.fabric_item_details | Job instances endpoint not found for item fbdd881a-7cf4-45d8-a32e-5e0290aa4a44\n",
      "2025-12-03 23:51:40 | WARNING | usf_fabric_monitoring.core.fabric_item_details | Job instances endpoint not found for item 92ecd049-df33-4d9c-bb0c-e6f39381f80c\n",
      "2025-12-03 23:51:40 | WARNING | usf_fabric_monitoring.core.fabric_item_details | Job instances endpoint not found for item 92ecd049-df33-4d9c-bb0c-e6f39381f80c\n",
      "2025-12-03 23:51:40 | WARNING | usf_fabric_monitoring.core.fabric_item_details | Job instances endpoint not found for item 5621a10a-fdb9-476e-b22c-2600eca8bd2a\n",
      "2025-12-03 23:51:40 | WARNING | usf_fabric_monitoring.core.fabric_item_details | Job instances endpoint not found for item 5621a10a-fdb9-476e-b22c-2600eca8bd2a\n",
      "2025-12-03 23:51:40 | WARNING | usf_fabric_monitoring.core.fabric_item_details | Job instances endpoint not found for item 8d945dca-e698-4027-a133-e33170ee422c\n",
      "2025-12-03 23:51:40 | WARNING | usf_fabric_monitoring.core.fabric_item_details | Job instances endpoint not found for item 8d945dca-e698-4027-a133-e33170ee422c\n",
      "2025-12-03 23:51:40 | WARNING | usf_fabric_monitoring.core.fabric_item_details | Job instances endpoint not found for item 1f14b8dd-7354-4785-823c-6e9ac124f3a3\n",
      "2025-12-03 23:51:40 | WARNING | usf_fabric_monitoring.core.fabric_item_details | Job instances endpoint not found for item 1f14b8dd-7354-4785-823c-6e9ac124f3a3\n",
      "2025-12-03 23:51:41 | WARNING | usf_fabric_monitoring.core.fabric_item_details | Job instances endpoint not found for item 7dad6889-d01d-48d6-b19c-1e1ef5ac6f2d\n",
      "2025-12-03 23:51:41 | WARNING | usf_fabric_monitoring.core.fabric_item_details | Job instances endpoint not found for item 7dad6889-d01d-48d6-b19c-1e1ef5ac6f2d\n",
      "2025-12-03 23:51:41 | WARNING | usf_fabric_monitoring.core.fabric_item_details | Job instances endpoint not found for item 0520ca63-e123-48c9-951c-9e7250787b4d\n",
      "2025-12-03 23:51:41 | WARNING | usf_fabric_monitoring.core.fabric_item_details | Job instances endpoint not found for item 0520ca63-e123-48c9-951c-9e7250787b4d\n",
      "2025-12-03 23:51:41 | WARNING | usf_fabric_monitoring.core.fabric_item_details | Job instances endpoint not found for item d70bd5c7-c8e3-468b-81ef-44c7601b9970\n",
      "2025-12-03 23:51:41 | WARNING | usf_fabric_monitoring.core.fabric_item_details | Job instances endpoint not found for item d70bd5c7-c8e3-468b-81ef-44c7601b9970\n",
      "2025-12-03 23:51:41 | WARNING | usf_fabric_monitoring.core.fabric_item_details | Job instances endpoint not found for item 552ac880-fcbe-40e9-b412-456a8c238901\n",
      "2025-12-03 23:51:41 | WARNING | usf_fabric_monitoring.core.fabric_item_details | Job instances endpoint not found for item 552ac880-fcbe-40e9-b412-456a8c238901\n",
      "2025-12-03 23:51:41 | WARNING | usf_fabric_monitoring.core.fabric_item_details | Job instances endpoint not found for item 74c09188-da5c-4e72-bf97-645e79a5327a\n",
      "2025-12-03 23:51:41 | WARNING | usf_fabric_monitoring.core.fabric_item_details | Job instances endpoint not found for item 74c09188-da5c-4e72-bf97-645e79a5327a\n",
      "2025-12-03 23:51:41 | WARNING | usf_fabric_monitoring.core.fabric_item_details | Job instances endpoint not found for item f445e410-a00d-4433-bf22-0d856d7ed5ef\n",
      "2025-12-03 23:51:41 | WARNING | usf_fabric_monitoring.core.fabric_item_details | Job instances endpoint not found for item f445e410-a00d-4433-bf22-0d856d7ed5ef\n",
      "2025-12-03 23:51:41 | WARNING | usf_fabric_monitoring.core.fabric_item_details | Job instances endpoint not found for item ffd25abe-ebae-4de8-b977-30ec6a3e4208\n",
      "2025-12-03 23:51:41 | INFO | fabric_item_details | Processing workspace: ABBA OSCAR Credit (0cb7fa46-aec2-4d66-95db-e4a09753732f)\n",
      "2025-12-03 23:51:41 | INFO | usf_fabric_monitoring.core.extractor | Fetching items for workspace 0cb7fa46-aec2-4d66-95db-e4a09753732f\n",
      "2025-12-03 23:51:41 | WARNING | usf_fabric_monitoring.core.fabric_item_details | Job instances endpoint not found for item ffd25abe-ebae-4de8-b977-30ec6a3e4208\n",
      "2025-12-03 23:51:41 | INFO | fabric_item_details | Processing workspace: ABBA OSCAR Credit (0cb7fa46-aec2-4d66-95db-e4a09753732f)\n",
      "2025-12-03 23:51:41 | INFO | usf_fabric_monitoring.core.extractor | Fetching items for workspace 0cb7fa46-aec2-4d66-95db-e4a09753732f\n",
      "2025-12-03 23:51:41 | INFO | usf_fabric_monitoring.core.extractor | Retrieved 6 items for workspace 0cb7fa46-aec2-4d66-95db-e4a09753732f\n",
      "2025-12-03 23:51:41 | INFO | usf_fabric_monitoring.core.extractor | Retrieved 6 items for workspace 0cb7fa46-aec2-4d66-95db-e4a09753732f\n",
      "2025-12-03 23:51:42 | WARNING | usf_fabric_monitoring.core.fabric_item_details | Job instances endpoint not found for item fe97536b-2ed3-4f24-8419-ebeedd20cdda\n",
      "2025-12-03 23:51:42 | WARNING | usf_fabric_monitoring.core.fabric_item_details | Job instances endpoint not found for item fe97536b-2ed3-4f24-8419-ebeedd20cdda\n",
      "2025-12-03 23:51:42 | WARNING | usf_fabric_monitoring.core.fabric_item_details | Job instances endpoint not found for item c369d433-c3b6-40fb-8510-5f77f6588c06\n",
      "2025-12-03 23:51:42 | WARNING | usf_fabric_monitoring.core.fabric_item_details | Job instances endpoint not found for item c369d433-c3b6-40fb-8510-5f77f6588c06\n",
      "2025-12-03 23:51:42 | WARNING | usf_fabric_monitoring.core.fabric_item_details | Job instances endpoint not found for item 98cd9399-5e96-4522-98f3-a2783cfdf4ee\n",
      "2025-12-03 23:51:42 | WARNING | usf_fabric_monitoring.core.fabric_item_details | Job instances endpoint not found for item 98cd9399-5e96-4522-98f3-a2783cfdf4ee\n",
      "2025-12-03 23:51:42 | WARNING | usf_fabric_monitoring.core.fabric_item_details | Job instances endpoint not found for item 0ad0d908-8522-4598-82ac-b07c5d82e6cd\n",
      "2025-12-03 23:51:42 | WARNING | usf_fabric_monitoring.core.fabric_item_details | Job instances endpoint not found for item 0ad0d908-8522-4598-82ac-b07c5d82e6cd\n",
      "2025-12-03 23:51:42 | ERROR | usf_fabric_monitoring.core.fabric_item_details | Failed to fetch job instances for item 81ddbac1-9b6c-4d4e-8919-32dd146f6f33: 400 Client Error: Bad Request for url: https://api.fabric.microsoft.com/v1/workspaces/0cb7fa46-aec2-4d66-95db-e4a09753732f/items/81ddbac1-9b6c-4d4e-8919-32dd146f6f33/jobs/instances\n",
      "2025-12-03 23:51:42 | ERROR | usf_fabric_monitoring.core.fabric_item_details | Failed to fetch job instances for item 81ddbac1-9b6c-4d4e-8919-32dd146f6f33: 400 Client Error: Bad Request for url: https://api.fabric.microsoft.com/v1/workspaces/0cb7fa46-aec2-4d66-95db-e4a09753732f/items/81ddbac1-9b6c-4d4e-8919-32dd146f6f33/jobs/instances\n",
      "2025-12-03 23:51:42 | ERROR | usf_fabric_monitoring.core.fabric_item_details | Failed to fetch job instances for item 2259b328-0f3d-4da6-bd09-c480734c0dea: 400 Client Error: Bad Request for url: https://api.fabric.microsoft.com/v1/workspaces/0cb7fa46-aec2-4d66-95db-e4a09753732f/items/2259b328-0f3d-4da6-bd09-c480734c0dea/jobs/instances\n",
      "2025-12-03 23:51:42 | INFO | fabric_item_details | Processing workspace: RE Corporate Planning - Balanced Scorecard [Test] (695bbafc-5a4b-489c-b795-4a5c345a1024)\n",
      "2025-12-03 23:51:42 | INFO | usf_fabric_monitoring.core.extractor | Fetching items for workspace 695bbafc-5a4b-489c-b795-4a5c345a1024\n",
      "2025-12-03 23:51:42 | ERROR | usf_fabric_monitoring.core.fabric_item_details | Failed to fetch job instances for item 2259b328-0f3d-4da6-bd09-c480734c0dea: 400 Client Error: Bad Request for url: https://api.fabric.microsoft.com/v1/workspaces/0cb7fa46-aec2-4d66-95db-e4a09753732f/items/2259b328-0f3d-4da6-bd09-c480734c0dea/jobs/instances\n",
      "2025-12-03 23:51:42 | INFO | fabric_item_details | Processing workspace: RE Corporate Planning - Balanced Scorecard [Test] (695bbafc-5a4b-489c-b795-4a5c345a1024)\n",
      "2025-12-03 23:51:42 | INFO | usf_fabric_monitoring.core.extractor | Fetching items for workspace 695bbafc-5a4b-489c-b795-4a5c345a1024\n",
      "2025-12-03 23:51:42 | INFO | usf_fabric_monitoring.core.extractor | Retrieved 10 items for workspace 695bbafc-5a4b-489c-b795-4a5c345a1024\n",
      "2025-12-03 23:51:42 | INFO | usf_fabric_monitoring.core.extractor | Retrieved 10 items for workspace 695bbafc-5a4b-489c-b795-4a5c345a1024\n",
      "2025-12-03 23:51:42 | WARNING | usf_fabric_monitoring.core.fabric_item_details | Job instances endpoint not found for item 938d1f5c-ab84-40ad-9cd7-7a3f6f200bf9\n",
      "2025-12-03 23:51:42 | WARNING | usf_fabric_monitoring.core.fabric_item_details | Job instances endpoint not found for item 938d1f5c-ab84-40ad-9cd7-7a3f6f200bf9\n",
      "2025-12-03 23:51:42 | WARNING | usf_fabric_monitoring.core.fabric_item_details | Job instances endpoint not found for item a92bfbe3-b08f-4f02-aca1-0bffc04a98d4\n",
      "2025-12-03 23:51:42 | WARNING | usf_fabric_monitoring.core.fabric_item_details | Job instances endpoint not found for item a92bfbe3-b08f-4f02-aca1-0bffc04a98d4\n",
      "2025-12-03 23:51:43 | WARNING | usf_fabric_monitoring.core.fabric_item_details | Job instances endpoint not found for item f734d822-7476-4a72-bbe6-ddff2971cb24\n",
      "2025-12-03 23:51:43 | WARNING | usf_fabric_monitoring.core.fabric_item_details | Job instances endpoint not found for item f734d822-7476-4a72-bbe6-ddff2971cb24\n",
      "2025-12-03 23:51:43 | WARNING | usf_fabric_monitoring.core.fabric_item_details | Job instances endpoint not found for item 815134ed-2bf1-4d70-8468-73b415658fdf\n",
      "2025-12-03 23:51:43 | WARNING | usf_fabric_monitoring.core.fabric_item_details | Job instances endpoint not found for item 815134ed-2bf1-4d70-8468-73b415658fdf\n",
      "2025-12-03 23:51:43 | WARNING | usf_fabric_monitoring.core.fabric_item_details | Job instances endpoint not found for item 09e51fab-44a0-4f0a-bcb2-92a79b0df0e7\n",
      "2025-12-03 23:51:43 | WARNING | usf_fabric_monitoring.core.fabric_item_details | Job instances endpoint not found for item 09e51fab-44a0-4f0a-bcb2-92a79b0df0e7\n",
      "2025-12-03 23:51:43 | WARNING | usf_fabric_monitoring.core.fabric_item_details | Job instances endpoint not found for item f32e4956-54ab-4c3f-99c0-6a5a013e6522\n",
      "2025-12-03 23:51:43 | WARNING | usf_fabric_monitoring.core.fabric_item_details | Job instances endpoint not found for item f32e4956-54ab-4c3f-99c0-6a5a013e6522\n",
      "2025-12-03 23:51:43 | ERROR | usf_fabric_monitoring.core.fabric_item_details | Failed to fetch job instances for item 21513193-8480-403e-9aed-8349439cbd10: 403 Client Error: Forbidden for url: https://api.fabric.microsoft.com/v1/workspaces/695bbafc-5a4b-489c-b795-4a5c345a1024/items/21513193-8480-403e-9aed-8349439cbd10/jobs/instances\n",
      "2025-12-03 23:51:43 | ERROR | usf_fabric_monitoring.core.fabric_item_details | Failed to fetch job instances for item 21513193-8480-403e-9aed-8349439cbd10: 403 Client Error: Forbidden for url: https://api.fabric.microsoft.com/v1/workspaces/695bbafc-5a4b-489c-b795-4a5c345a1024/items/21513193-8480-403e-9aed-8349439cbd10/jobs/instances\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m pipeline = MonitorHubPipeline(OUTPUT_DIR)\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m results = \u001b[43mpipeline\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_complete_analysis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdays\u001b[49m\u001b[43m=\u001b[49m\u001b[43mDAYS_TO_ANALYZE\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      3\u001b[39m pipeline.print_results_summary(results)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/fabric-monitoring/lib/python3.11/site-packages/usf_fabric_monitoring/core/pipeline.py:78\u001b[39m, in \u001b[36mMonitorHubPipeline.run_complete_analysis\u001b[39m\u001b[34m(self, days)\u001b[39m\n\u001b[32m     75\u001b[39m \u001b[38;5;66;03m# We use a separate directory for item details, but we can pass it if needed.\u001b[39;00m\n\u001b[32m     76\u001b[39m \u001b[38;5;66;03m# The default is exports/fabric_item_details, which matches what we load later.\u001b[39;00m\n\u001b[32m     77\u001b[39m details_output_dir = \u001b[38;5;28mself\u001b[39m.output_directory / \u001b[33m\"\u001b[39m\u001b[33mfabric_item_details\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m78\u001b[39m details_result = \u001b[43mrun_item_details_extraction\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     79\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdetails_output_dir\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     80\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     82\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m details_result.get(\u001b[33m\"\u001b[39m\u001b[33mstatus\u001b[39m\u001b[33m\"\u001b[39m) != \u001b[33m\"\u001b[39m\u001b[33msuccess\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m     83\u001b[39m     \u001b[38;5;28mself\u001b[39m.logger.warning(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mDetailed job extraction failed: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdetails_result.get(\u001b[33m'\u001b[39m\u001b[33mmessage\u001b[39m\u001b[33m'\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/fabric-monitoring/lib/python3.11/site-packages/usf_fabric_monitoring/scripts/extract_fabric_item_details.py:143\u001b[39m, in \u001b[36mrun_item_details_extraction\u001b[39m\u001b[34m(workspace_id, output_dir, log_level)\u001b[39m\n\u001b[32m    140\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    141\u001b[39m     \u001b[38;5;66;03m# Try to fetch jobs for all other item types\u001b[39;00m\n\u001b[32m    142\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m143\u001b[39m         jobs = \u001b[43mdetail_extractor\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_item_job_instances\u001b[49m\u001b[43m(\u001b[49m\u001b[43mws_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mitem_id\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    144\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m jobs:\n\u001b[32m    145\u001b[39m             \u001b[38;5;66;03m# Filter jobs if incremental run\u001b[39;00m\n\u001b[32m    146\u001b[39m             \u001b[38;5;28;01mif\u001b[39;00m last_processed_time:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/fabric-monitoring/lib/python3.11/site-packages/usf_fabric_monitoring/core/fabric_item_details.py:67\u001b[39m, in \u001b[36mFabricItemDetailExtractor.get_item_job_instances\u001b[39m\u001b[34m(self, workspace_id, item_id)\u001b[39m\n\u001b[32m     63\u001b[39m headers = \u001b[38;5;28mself\u001b[39m.auth.get_fabric_headers()\n\u001b[32m     65\u001b[39m \u001b[38;5;28mself\u001b[39m.logger.debug(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFetching job instances for item \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mitem_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m in workspace \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mworkspace_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m67\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msession\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     69\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m response.status_code == \u001b[32m404\u001b[39m:\n\u001b[32m     70\u001b[39m     \u001b[38;5;28mself\u001b[39m.logger.warning(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mJob instances endpoint not found for item \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mitem_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/fabric-monitoring/lib/python3.11/site-packages/requests/sessions.py:602\u001b[39m, in \u001b[36mSession.get\u001b[39m\u001b[34m(self, url, **kwargs)\u001b[39m\n\u001b[32m    594\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"Sends a GET request. Returns :class:`Response` object.\u001b[39;00m\n\u001b[32m    595\u001b[39m \n\u001b[32m    596\u001b[39m \u001b[33;03m:param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[32m    597\u001b[39m \u001b[33;03m:param \\*\\*kwargs: Optional arguments that ``request`` takes.\u001b[39;00m\n\u001b[32m    598\u001b[39m \u001b[33;03m:rtype: requests.Response\u001b[39;00m\n\u001b[32m    599\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    601\u001b[39m kwargs.setdefault(\u001b[33m\"\u001b[39m\u001b[33mallow_redirects\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m--> \u001b[39m\u001b[32m602\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mGET\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/fabric-monitoring/lib/python3.11/site-packages/requests/sessions.py:589\u001b[39m, in \u001b[36mSession.request\u001b[39m\u001b[34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[39m\n\u001b[32m    584\u001b[39m send_kwargs = {\n\u001b[32m    585\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mtimeout\u001b[39m\u001b[33m\"\u001b[39m: timeout,\n\u001b[32m    586\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mallow_redirects\u001b[39m\u001b[33m\"\u001b[39m: allow_redirects,\n\u001b[32m    587\u001b[39m }\n\u001b[32m    588\u001b[39m send_kwargs.update(settings)\n\u001b[32m--> \u001b[39m\u001b[32m589\u001b[39m resp = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    591\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/fabric-monitoring/lib/python3.11/site-packages/requests/sessions.py:703\u001b[39m, in \u001b[36mSession.send\u001b[39m\u001b[34m(self, request, **kwargs)\u001b[39m\n\u001b[32m    700\u001b[39m start = preferred_clock()\n\u001b[32m    702\u001b[39m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m703\u001b[39m r = \u001b[43madapter\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    705\u001b[39m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[32m    706\u001b[39m elapsed = preferred_clock() - start\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/fabric-monitoring/lib/python3.11/site-packages/requests/adapters.py:644\u001b[39m, in \u001b[36mHTTPAdapter.send\u001b[39m\u001b[34m(self, request, stream, timeout, verify, cert, proxies)\u001b[39m\n\u001b[32m    641\u001b[39m     timeout = TimeoutSauce(connect=timeout, read=timeout)\n\u001b[32m    643\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m644\u001b[39m     resp = \u001b[43mconn\u001b[49m\u001b[43m.\u001b[49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    645\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    646\u001b[39m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    647\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    648\u001b[39m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    649\u001b[39m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    650\u001b[39m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    651\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    652\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    653\u001b[39m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    654\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    655\u001b[39m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    656\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    658\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[32m    659\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(err, request=request)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/fabric-monitoring/lib/python3.11/site-packages/urllib3/connectionpool.py:940\u001b[39m, in \u001b[36mHTTPConnectionPool.urlopen\u001b[39m\u001b[34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[39m\n\u001b[32m    937\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m response\n\u001b[32m    939\u001b[39m response.drain_conn()\n\u001b[32m--> \u001b[39m\u001b[32m940\u001b[39m \u001b[43mretries\u001b[49m\u001b[43m.\u001b[49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    941\u001b[39m log.debug(\u001b[33m\"\u001b[39m\u001b[33mRetry: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m, url)\n\u001b[32m    942\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.urlopen(\n\u001b[32m    943\u001b[39m     method,\n\u001b[32m    944\u001b[39m     url,\n\u001b[32m   (...)\u001b[39m\u001b[32m    957\u001b[39m     **response_kw,\n\u001b[32m    958\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/fabric-monitoring/lib/python3.11/site-packages/urllib3/util/retry.py:359\u001b[39m, in \u001b[36mRetry.sleep\u001b[39m\u001b[34m(self, response)\u001b[39m\n\u001b[32m    350\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Sleep between retry attempts.\u001b[39;00m\n\u001b[32m    351\u001b[39m \n\u001b[32m    352\u001b[39m \u001b[33;03mThis method will respect a server's ``Retry-After`` response header\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    355\u001b[39m \u001b[33;03mthis method will return immediately.\u001b[39;00m\n\u001b[32m    356\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    358\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.respect_retry_after_header \u001b[38;5;129;01mand\u001b[39;00m response:\n\u001b[32m--> \u001b[39m\u001b[32m359\u001b[39m     slept = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msleep_for_retry\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    360\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m slept:\n\u001b[32m    361\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/fabric-monitoring/lib/python3.11/site-packages/urllib3/util/retry.py:338\u001b[39m, in \u001b[36mRetry.sleep_for_retry\u001b[39m\u001b[34m(self, response)\u001b[39m\n\u001b[32m    336\u001b[39m retry_after = \u001b[38;5;28mself\u001b[39m.get_retry_after(response)\n\u001b[32m    337\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m retry_after:\n\u001b[32m--> \u001b[39m\u001b[32m338\u001b[39m     \u001b[43mtime\u001b[49m\u001b[43m.\u001b[49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mretry_after\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    339\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    341\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "pipeline = MonitorHubPipeline(OUTPUT_DIR)\n",
    "results = pipeline.run_complete_analysis(days=DAYS_TO_ANALYZE)\n",
    "pipeline.print_results_summary(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a6c61f0",
   "metadata": {},
   "source": [
    "## 5. Advanced Analysis & Visualization (Spark)\n",
    "The following cells use PySpark to load the raw data generated by the pipeline and provide interactive visualizations of failures, error codes, and trends."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aad9e0ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Setup Spark & Paths\n",
    "import os\n",
    "import glob\n",
    "from usf_fabric_monitoring.core.utils import resolve_path\n",
    "\n",
    "# Initialize Spark Session (if not already active)\n",
    "spark = None\n",
    "try:\n",
    "    from pyspark.sql import SparkSession\n",
    "    from pyspark.sql.functions import col, to_timestamp, when, count, desc, lit, unix_timestamp, coalesce, abs as abs_val, split, initcap, regexp_replace, element_at, substring, avg, max, min\n",
    "    from pyspark.sql.types import StructType, StructField, StringType, DoubleType\n",
    "\n",
    "    if 'spark' not in locals() or spark is None:\n",
    "        print(\"‚öôÔ∏è Initializing Spark Session...\")\n",
    "        spark = SparkSession.builder \\\n",
    "            .appName(\"FabricFailureAnalysis\") \\\n",
    "            .getOrCreate()\n",
    "        print(f\"‚úÖ Spark Session Created: {spark.version}\")\n",
    "except ImportError:\n",
    "    print(\"‚ö†Ô∏è PySpark not installed or configured. Skipping Spark-based analysis.\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Failed to initialize Spark: {e}. Skipping Spark-based analysis.\")\n",
    "\n",
    "# Resolve the output directory to an absolute path\n",
    "# This ensures that if you used a relative path like \"monitor_hub_analysis\",\n",
    "# it is correctly resolved to \"/lakehouse/default/Files/monitor_hub_analysis\" for Spark.\n",
    "resolved_output_dir = str(resolve_path(OUTPUT_DIR))\n",
    "\n",
    "BASE_PATH = os.path.join(resolved_output_dir, \"fabric_item_details\")\n",
    "AUDIT_LOG_PATH = os.path.join(resolved_output_dir, \"raw_data/daily\")\n",
    "\n",
    "print(f\"üìÇ Analysis Paths:\")\n",
    "print(f\"  - Item Details: {BASE_PATH}\")\n",
    "print(f\"  - Audit Logs:   {AUDIT_LOG_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cec636d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Load Data from Parquet (Source of Truth)\n",
    "\n",
    "import os\n",
    "from pyspark.sql.functions import col, to_timestamp, unix_timestamp, coalesce, initcap, regexp_replace, element_at, split, when, lit\n",
    "\n",
    "PARQUET_PATH = os.path.join(resolved_output_dir, \"parquet\")\n",
    "\n",
    "def load_parquet_data():\n",
    "    \"\"\"Loads the enriched activity data from Parquet files.\"\"\"\n",
    "    try:\n",
    "        path_pattern = os.path.join(PARQUET_PATH, \"activities_*.parquet\")\n",
    "        print(f\"üìÇ Loading Parquet files from {path_pattern}...\")\n",
    "        \n",
    "        # Read Parquet\n",
    "        df = spark.read.parquet(path_pattern)\n",
    "        \n",
    "        # Filter for Failures (checking both case conventions)\n",
    "        # Detailed jobs use 'status', raw logs use 'Status'\n",
    "        # We check if columns exist before filtering to avoid AnalysisException\n",
    "        cols = df.columns\n",
    "        conditions = []\n",
    "        if \"status\" in cols:\n",
    "            conditions.append(col(\"status\") == \"Failed\")\n",
    "        if \"Status\" in cols:\n",
    "            conditions.append(col(\"Status\") == \"Failed\")\n",
    "            \n",
    "        if conditions:\n",
    "            from functools import reduce\n",
    "            # Combine conditions with OR\n",
    "            failed_df = df.filter(reduce(lambda x, y: x | y, conditions))\n",
    "            return failed_df\n",
    "        else:\n",
    "            print(\"‚ö†Ô∏è 'status' column not found in Parquet data.\")\n",
    "            return df # Return all if status not found, or empty?\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Could not load Parquet data: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# Execute Loading\n",
    "final_df = load_parquet_data()\n",
    "\n",
    "if final_df:\n",
    "    print(f\"‚úÖ Successfully loaded {final_df.count()} failure records from Parquet.\")\n",
    "    \n",
    "    # Handle mixed schema (snake_case from detailed jobs vs PascalCase from raw logs)\n",
    "    # Detailed jobs (snake_case) are the primary source for failure details.\n",
    "    \n",
    "    # Helper to safely get column or null\n",
    "    def safe_col(c):\n",
    "        return col(c) if c in final_df.columns else lit(None)\n",
    "\n",
    "    final_df = final_df.select(\n",
    "        coalesce(safe_col(\"workspace_name\"), safe_col(\"WorkSpaceName\")).alias(\"Workspace\"),\n",
    "        coalesce(safe_col(\"item_name\"), safe_col(\"ItemName\")).alias(\"Item Name\"),\n",
    "        coalesce(safe_col(\"item_type\"), safe_col(\"ItemType\")).alias(\"Item Type\"),\n",
    "        coalesce(safe_col(\"activity_type\"), safe_col(\"Operation\")).alias(\"Invoke Type\"),\n",
    "        coalesce(safe_col(\"start_time\"), safe_col(\"CreationTime\")).alias(\"Start Time\"),\n",
    "        coalesce(safe_col(\"end_time\"), safe_col(\"EndTime\")).alias(\"End Time\"),\n",
    "        coalesce(safe_col(\"duration\"), safe_col(\"Duration\")).alias(\"Duration (s)\"),\n",
    "        coalesce(safe_col(\"submitted_by\"), safe_col(\"UserId\")).alias(\"User ID\"),\n",
    "        \n",
    "        # User Name Extraction\n",
    "        coalesce(\n",
    "            initcap(regexp_replace(element_at(split(coalesce(safe_col(\"submitted_by\"), safe_col(\"UserId\")), \"@\"), 1), \"\\\\.\", \" \")),\n",
    "            safe_col(\"submitted_by\"), \n",
    "            safe_col(\"UserId\")\n",
    "        ).alias(\"User Name\"),\n",
    "        \n",
    "        # Error Details (Try to get from failure_reason struct)\n",
    "        safe_col(\"failure_reason.errorCode\").alias(\"Error Code\"),\n",
    "        safe_col(\"failure_reason.message\").alias(\"Error Message\")\n",
    "    )\n",
    "else:\n",
    "    print(\"‚ùå No failure data found.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b851589",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Analysis & Display\n",
    "\n",
    "if final_df:\n",
    "    # --- 1. Summary Statistics ---\n",
    "    total_failures = final_df.count()\n",
    "    unique_workspaces = final_df.select(\"Workspace\").distinct().count()\n",
    "    unique_items = final_df.select(\"Item Name\").distinct().count()\n",
    "    \n",
    "    print(f\"\\nüìä SUMMARY STATISTICS\")\n",
    "    print(f\"Total Failures: {total_failures}\")\n",
    "    print(f\"Affected Workspaces: {unique_workspaces}\")\n",
    "    print(f\"Affected Items: {unique_items}\")\n",
    "\n",
    "    # --- 2. Top 10 Failing Items ---\n",
    "    print(\"\\nüèÜ TOP 10 FAILING ITEMS\")\n",
    "    top_items = final_df.groupBy(\"Workspace\", \"Item Name\", \"Item Type\") \\\n",
    "        .count() \\\n",
    "        .orderBy(col(\"count\").desc()) \\\n",
    "        .limit(10)\n",
    "    top_items.show(truncate=False)\n",
    "\n",
    "    # --- 3. Failures by User ---\n",
    "    print(\"\\nüë§ FAILURES BY USER\")\n",
    "    user_stats = final_df.groupBy(\"User Name\") \\\n",
    "        .count() \\\n",
    "        .orderBy(col(\"count\").desc())\n",
    "    user_stats.show(truncate=False)\n",
    "\n",
    "    # --- 4. Error Code Distribution ---\n",
    "    print(\"\\n‚ö†Ô∏è ERROR CODE DISTRIBUTION\")\n",
    "    error_stats = final_df.groupBy(\"Error Code\") \\\n",
    "        .count() \\\n",
    "        .orderBy(col(\"count\").desc())\n",
    "    error_stats.show(truncate=False)\n",
    "\n",
    "    # --- 5. Recent Failures (Last 20) ---\n",
    "    print(\"\\nüïí MOST RECENT FAILURES\")\n",
    "    final_df.select(\"Start Time\", \"Workspace\", \"Item Name\", \"User Name\", \"Error Message\") \\\n",
    "        .orderBy(col(\"Start Time\").desc()) \\\n",
    "        .show(20, truncate=50)\n",
    "else:\n",
    "    print(\"No data available for analysis.\")"
   ]
  }
 ],
 "metadata": {
  "dependencies": {
   "environment": {
    "environmentId": "e818f402-5fea-490c-9eeb-2e9258522102",
    "workspaceId": "944ae69e-1c99-4ad7-973f-c296c778a5c5"
   },
   "lakehouse": {
    "default_lakehouse": "5cb78b9b-b153-4771-b309-65ec4848433a",
    "default_lakehouse_name": "lh_01_bronze",
    "default_lakehouse_workspace_id": "944ae69e-1c99-4ad7-973f-c296c778a5c5",
    "known_lakehouses": [
     {
      "id": "5cb78b9b-b153-4771-b309-65ec4848433a"
     }
    ]
   }
  },
  "kernel_info": {
   "name": "synapse_pyspark"
  },
  "kernelspec": {
   "display_name": "fabric-monitoring",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  },
  "microsoft": {
   "language": "python",
   "language_group": "synapse_pyspark",
   "ms_spell_check": {
    "ms_spell_check_language": "en"
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  },
  "spark_compute": {
   "compute_id": "/trident/default",
   "session_options": {
    "conf": {
     "spark.synapse.nbs.session.timeout": "1200000"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
